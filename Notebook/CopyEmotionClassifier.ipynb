{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beca2eb3-8614-451a-993d-d7133590e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np # NUMPY\n",
    "import pandas as pd # PANDAS\n",
    "import matplotlib.pyplot as plt # MATPLOTLIB\n",
    "import cv2\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752813c5-52a9-4b54-9726-7d6fc0272ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run auxfunctions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ebccb8c-b96d-491f-a9b5-8ad67dbf727b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn-som in /home/imagenes/anaconda3/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy in /home/imagenes/anaconda3/lib/python3.11/site-packages (from sklearn-som) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "#%run ../universityfiles/ExpressionMapFunctionsAux.ipynb\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c812d60a-54b8-4069-993a-277210c2c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run ../universityfiles/SOM2ver.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f213d5-cb1e-4906-b8f9-c3ef2c03b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f43a5e2-05e3-4611-b335-a9145808db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "#!chmod a+x '../cartoonTexture'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e93bc11-e447-49cd-b3e5-ebab2716d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doPCA(data, comp):\n",
    "    std_scaler = StandardScaler()\n",
    "    scaler = std_scaler.fit(data)\n",
    "    scaled_landDiff = scaler.transform(data)\n",
    "    pca = PCA(n_components=comp)\n",
    "    pca.fit_transform(scaled_landDiff)\n",
    "    b_shape = pca.fit_transform(scaled_landDiff)\n",
    "    print(sum(pca.explained_variance_ratio_))\n",
    "    print(b_shape.shape)\n",
    "    return b_shape, pca, scaler\n",
    "\n",
    "def transformPCA(data, pca, scaler):    \n",
    "    scaled_landDiff = std_scaler.transform(data)\n",
    "    return pca.transform(scaled_landDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4aa800b-a256-410e-86fd-815751356820",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Vamos a necesitar esto para los V1,V2,V3 anticuados. Usarlo y luego dejarlo comentado\n",
    "def discard_all_extra_neutrals(data, path_folder):\n",
    "  data = np.asarray(data)\n",
    "  image_names = np.sort(os.listdir(path_folder))\n",
    "  current_neutral_index = -1\n",
    "  my_data_for_PCA_SNeu = []\n",
    "  current_subject = 'start'\n",
    "\n",
    "  for i in range(0, len(images_names)):\n",
    "    if int(images_names[i].split('_')[5].split('.')[0]) == 1:\n",
    "      if current_subject != images_names[i].split('_')[3]:\n",
    "        current_subject = images_names[i].split('_')[3]\n",
    "        my_data_for_PCA_SNeu.append(data[i])\n",
    "    else:\n",
    "        my_data_for_PCA_SNeu.append(data[i])\n",
    "\n",
    "  my_data_for_PCA_SNeu = np.asarray(my_data_for_PCA_SNeu)\n",
    "  return my_data_for_PCA_SNeu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "939c031d-8f01-420b-a983-ca5863cd75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_transform_datavector_of_images(reference_image, reference_landmarks, landmarks, images, pathi):\n",
    "    transformed_landmarks_total = []\n",
    "    for i in range(0, len(landmarks)):        \n",
    "        tr_Y_img, Z_pts = align_and_transform_image(reference_image, reference_landmarks, landmarks.iloc[i], images[i])\n",
    "        transformed_landmarks_total.append(Z_pts)\n",
    "        filename = 'warped_image_procustres_'+ str(images[i].split('/')[6])\n",
    "        path = pathi\n",
    "        cv2.imwrite(os.path.join(path , filename), tr_Y_img)\n",
    "    transformed_landmarks_total = np.asarray(transformed_landmarks_total)\n",
    "    return transformed_landmarks_total\n",
    "\n",
    "def get_inten_emo(my_data_2):\n",
    "  Ai = my_data_2.max()\n",
    "  data_2_intensity = my_data_2\n",
    "  data_2_intensity_div = data_2_intensity/Ai\n",
    "  data_2_intensity_sum =  np.sum(data_2_intensity_div, axis=1)\n",
    "  data_2_intensity_final = data_2_intensity_sum/136\n",
    "  return data_2_intensity_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab1486c7-e4d6-4f49-81e3-b5bbf2d61121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_struct_no_neu(my_data_2, data_2_intensity_final, structure_vector, no_neutral_im):\n",
    "    cant_features = 22\n",
    "    current_sub = 'start'    \n",
    "    ab = 0\n",
    "    structure_feature_vector = np.zeros((len(my_data_2), cant_features))\n",
    "    current_features_index = -1\n",
    "    for i in range(0, len(data_2_intensity_final)):        \n",
    "        if  no_neutral_im[ab].split('_')[3] != current_sub:\n",
    "            current_sub = no_neutral_im[ab].split('_')[3]\n",
    "            current_features_index = current_features_index + 1\n",
    "        vector = structure_vector[:,current_features_index]#columna i\n",
    "        vector = np.append(vector,data_2_intensity_final.iloc[i])\n",
    "        structure_feature_vector[ab] = vector\n",
    "        ab = ab + 1\n",
    "    return structure_feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a95207-66a4-479a-9bc3-24de09819a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartoon_texture_decomposition(images_path, subject_images, cartoon_folder_path, texture_folder_path, j=0.2):\n",
    "    for i in range (0,  len(subject_images)):  \n",
    "        subject = subject_images[i].split('/')[0].split('_')[3]\n",
    "        serie = subject_images[i].split('/')[0].split('_')[4]\n",
    "        frame = subject_images[i].split('/')[0].split('_')[5].split('.')[0]\n",
    "        print(subject,serie, frame)\n",
    "        image_path = images_path + '/' + subject_images[i]\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img2 = img[40:450, 170:580]\n",
    "        cv2.imwrite(cropped_img_path, img2)\n",
    "        cartoon_path = cartoon_folder_path + \"/cartoonimg\"+'_'+ str(int(j*100))+'_' + subject +'_' +serie +'_'+frame +\".png\"\n",
    "        texture_path = texture_folder_path +\"/textureimg\"+'_'+ str(int(j*100))+'_' + subject +'_' +serie +'_'+frame +\".png\"\n",
    "        subprocess.check_output(['../cartoonTexture',cropped_img_path, str(j), cartoon_path, texture_path])\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d114241d-11d6-4783-baf9-23c9e6852adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postcartoon(cartimlist):\n",
    "  imcart = []\n",
    "  for i in cartimlist:\n",
    "    img = cv2.imread(i, cv2.IMREAD_GRAYSCALE)\n",
    "    img_mean = img/np.mean(img)\n",
    "    imcart.append(img_mean.flatten())\n",
    "  return imcart\n",
    "\n",
    "def diffpostcartoon(cartarray, cartimlist):\n",
    "  imcartoon = []\n",
    "  index = -1\n",
    "  subject = 'start'\n",
    "  current_image = cartarray[0]\n",
    "  for i in range(0, len(cartimlist)):    \n",
    "    if (int(cartimlist[i].split('/')[2].split('_')[4].split('.')[0]) == 1):\n",
    "      index = i\n",
    "      current_image = cartarray[i]\n",
    "    imcartoon.append(cartarray[i]- current_image)\n",
    "  return imcartoon\n",
    "\n",
    "def eliminate_extraneutrals(cartimlist, cartoondiff):\n",
    "  no_neutral_extra_im = []\n",
    "  cheking_order = []\n",
    "  current_sub = 'start'\n",
    "  for i in range (0, len(cartimlist)):\n",
    "    if cartimlist[i].split('_')[2] != current_sub:\n",
    "      current_sub = cartimlist[i].split('_')[2]\n",
    "      no_neutral_extra_im.append(cartoondiff[i])\n",
    "      cheking_order.append(cartimlist[i])\n",
    "    if int(cartimlist[i].split('_')[4].split('.')[0]) != 1:\n",
    "      no_neutral_extra_im.append(cartoondiff[i])\n",
    "      cheking_order.append(cartimlist[i])\n",
    "  no_neutral_extra_im = np.asarray(no_neutral_extra_im)\n",
    "  no_neutral_extra_im.shape\n",
    "  return no_neutral_extra_im, cheking_order\n",
    "\n",
    "def posttexture(teximlist):\n",
    "  imtext = []\n",
    "  for i in teximlist:\n",
    "    img = cv2.imread(i, cv2.IMREAD_GRAYSCALE)\n",
    "    imtext.append(img.flatten())\n",
    "\n",
    "  imtextarray = np.asarray(imtext)\n",
    "  imtextarray.shape\n",
    "  return imtextarray\n",
    "\n",
    "def struct_read(path, chunk_size = 100):\n",
    "  file_path = path      \n",
    "  df_list = []\n",
    "  for chunk in pd.read_csv(file_path, chunksize=chunk_size, header = None):\n",
    "    df_list.append(chunk)\n",
    "  df = pd.concat(df_list)\n",
    "  return df  #esto equivale a tirar por balde el ahorro. creo\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "613ebcd6-a0c5-4320-b80e-e029ba1424bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier():\n",
    "    \"\"\"\n",
    "    The 2-D, rectangular grid self-organizing map class using Numpy.\n",
    "    \"\"\"\n",
    "    def __init__(self, training_data_images, training_data_landmarks, aligned_training_images_path, aligned_training_landmarks_path, shape_diff_path,  m=9, n=9, lr=0.25, sigma=1.75):\n",
    "\n",
    "\n",
    "        # Initialize descriptive features of SOM\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.shape = (m, n)\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.training_set_img_folder = training_data_images        \n",
    "        self.training_set_landmarks_folder = training_data_landmarks       \n",
    "\n",
    "        self.aligned_training_images_path = aligned_training_images_path\n",
    "        self.aligned_training_landmarks_path = aligned_training_landmarks_path\n",
    "        self.shape_diff_path = shape_diff_path\n",
    "        self.aligned_test_images_path = None\n",
    "        self.aligned_test_landmarks_path = None\n",
    "        self.shape_diff_path_test = None\n",
    "\n",
    "        # Initialize weights\n",
    "        self.shape_pca = None\n",
    "        self.structure_pca = None\n",
    "        self.cartoon_pca = None\n",
    "        self.texture_pca = None\n",
    "\n",
    "        self.shape_scaler = None\n",
    "        self.structure_scaler = None\n",
    "        self.cartoon_scaler = None\n",
    "        self.texture_scaler = None\n",
    "\n",
    "        self.training_set_features = None\n",
    "        self.test_set_features = None\n",
    "\n",
    "\n",
    "\n",
    "        # Set after fitting\n",
    "\n",
    "\n",
    "    def _change_training_img_loc(self, loc):\n",
    "        \"\"\"\n",
    "        Return the indices of an m by n array.\n",
    "        \"\"\"\n",
    "        self.training_set_img_folder = loc\n",
    "        return self.training_set_img_folder\n",
    "\n",
    "    def _change_test_img_loc(self, loc):\n",
    "        \"\"\"\n",
    "        Return the indices of an m by n array.\n",
    "        \"\"\"\n",
    "        self.test_set_img_folder = loc\n",
    "        return self.test_set_img_folder\n",
    "\n",
    "    def _change_training_lan_loc(self, loc):\n",
    "        \"\"\"\n",
    "        Return the indices of an m by n array.\n",
    "        \"\"\"\n",
    "        self.training_set_landmarks_folder = loc\n",
    "        return self.training_set_landmarks_folder\n",
    "\n",
    "    def _change_test_lan_loc(self, loc):\n",
    "        \"\"\"\n",
    "        Return the indices of an m by n array.\n",
    "        \"\"\"\n",
    "        self.test_set_landmarks_folder = loc\n",
    "        return self.test_set_landmarks_folder\n",
    "\n",
    "    def _save_shape_pca(self):\n",
    "      pk.dump(self.shape_pca, open(\"shape_pca.pkl\",\"wb\"))\n",
    "\n",
    "    def _load_shape_pca(self):\n",
    "      self.shape_pca = pk.load(open(\"shape_pca.pkl\",\"rb\"))\n",
    "      return self.shape_pca\n",
    "\n",
    "    def _save_shape_scaler(self):\n",
    "      pk.dump(self.shape_scaler, open(\"shape_scaler.pkl\",\"wb\"))\n",
    "\n",
    "    def _load_shape_scaler(self):\n",
    "      self.shape_scaler = pk.load(open(\"shape_scaler.pkl\",\"rb\"))\n",
    "      return self.shape_scaler\n",
    "\n",
    "    def _save_structure_pca(self):\n",
    "      pk.dump(self.structure_pca, open(\"structure_pca.pkl\",\"wb\"))\n",
    "\n",
    "    def _load_structure_pca(self):\n",
    "      self.strucure_pca = pk.load(open(\"structure_pca.pkl\",\"rb\"))\n",
    "      return self.strucure_pca\n",
    "\n",
    "    def _save_structure_scaler(self):\n",
    "      pk.dump(self.structure_scaler, open(\"structure_scaler.pkl\",\"wb\"))\n",
    "\n",
    "    def _load_structure_scaler(self):\n",
    "      self.strucure_scaler = pk.load(open(\"structure_scaler.pkl\",\"rb\"))\n",
    "      return self.strucure_scaler\n",
    "        \n",
    "    def _save_cartoon_pca(self):\n",
    "      pk.dump(self.cartoon_pca, open(\"cartoon_pca.pkl\",\"wb\"))\n",
    "\n",
    "    def _load_cartoon_pca(self):\n",
    "      self.cartoon_pca = pk.load(open(\"cartoon_pca.pkl\",\"rb\"))\n",
    "      return self.cartoon_pca\n",
    "        \n",
    "    def _save_cartoon_scaler(self):\n",
    "      pk.dump(self.cartoon_scaler, open(\"cartoon_scaler.pkl\",\"wb\"))\n",
    "\n",
    "    def _load_cartoon_scaler(self):\n",
    "      self.cartoon_scaler = pk.load(open(\"cartoon_scaler.pkl\",\"rb\"))\n",
    "      return self.cartoon_scaler\n",
    "        \n",
    "    def _save_texture_pca(self):\n",
    "      pk.dump(self.texture_pca, open(\"texture_pca.pkl\",\"wb\"))\n",
    "\n",
    "    def _load_texture_pca(self):\n",
    "      self.texture_pca = pk.load(open(\"texture_pca.pkl\",\"rb\"))\n",
    "      return self.texture_pca\n",
    "        \n",
    "    def _save_texture_scaler(self):\n",
    "      pk.dump(self.texture_scaler, open(\"texture_scaler.pkl\",\"wb\"))\n",
    "\n",
    "    def _load_texture_pca(self):\n",
    "      self.texture_scaler = pk.load(open(\"texture_scaler.pkl\",\"rb\"))\n",
    "      return self.texture_scaler\n",
    "\n",
    "    \n",
    "    def _get_training_shape_feature(self, pca_comp = 29):\n",
    "\n",
    "      images = np.sort(np.asarray(self.training_set_img_folder))\n",
    "      landmarks = np.sort(np.asarray(self.training_set_landmarks_folder))\n",
    "      frames_lmarks_XYord = get_landmarks_of_folder(landmarks)\n",
    "      my_df = pd.DataFrame(frames_lmarks_XYord)\n",
    "      intercalated_data = intercalate_data_of_datavectors(my_df, 0)\n",
    "      my_dfc = pd.DataFrame(intercalated_data)\n",
    "\n",
    "      my_landmarks = my_dfc\n",
    "      pathi = self.aligned_training_images_path\n",
    "      transformed_landmarks_total = align_and_transform_datavector_of_images(images[0], my_landmarks.iloc[0], my_landmarks, images, pathi)\n",
    "\n",
    "      transformed_final = transformed_landmarks_total[0].flatten('F')\n",
    "      for i in range (1, len(transformed_landmarks_total)):\n",
    "        transformed_final = np.vstack((transformed_final, transformed_landmarks_total[i].flatten('F')))\n",
    "\n",
    "      my_df = pd.DataFrame(transformed_final)\n",
    "      my_df.to_csv(self.aligned_training_landmarks_path,header = False, index= False)\n",
    "\n",
    "      ind = 0\n",
    "      my_data_aligned_diff = []\n",
    "      images_names = images\n",
    "      current_neutral_index = 0\n",
    "      for i in range(0, len(images_names)):       \n",
    "        if int(images_names[i].split('_')[2].split('.')[0]) == 1:\n",
    "          current_neutral_index = i\n",
    "          #print('cambie neutral '+ str(current_neutral_index))\n",
    "        my_data_aligned_diff.append(my_df.iloc[i]- my_df.iloc[current_neutral_index])\n",
    "      my_data_aligned_diff = np.asarray(my_data_aligned_diff)\n",
    "\n",
    "      my_data_forPCA = pd.DataFrame(my_data_aligned_diff)\n",
    "      my_data_forPCA.to_csv(self.shape_diff_path,header = False, index= False)\n",
    "\n",
    "      shape,pac, scaler= doPCA(my_data_forPCA, pca_comp)\n",
    "\n",
    "      self.shape_pca = pac\n",
    "      self.shape_scaler = scaler\n",
    "      return shape\n",
    "    \n",
    "    def _get_training_structure_feature(self, structure_features_path, pca_comp = 13):\n",
    "      my_data = pd.read_csv(self.aligned_training_landmarks_path,header = None)\n",
    "      subjects_images = self.aligned_training_images_path      \n",
    "      first_frames = get_first_frames(subjects_images,my_data)\n",
    "      first_images = get_first_images(subjects_images,my_data)\n",
    "      intercalated_first_frames = change_to_intercalate_order(first_frames)\n",
    "      structure_vector = do_the_structure_points_and_stack(intercalated_first_frames, first_images)\n",
    "      my_data_2 = pd.read_csv(self.shape_diff_path,header = None)  \n",
    "      data_2_intensity_final = get_inten_emo(my_data_2)  \n",
    "      images_names = np.sort(os.listdir(self.aligned_training_images_path))\n",
    "        \n",
    "      structure_feature_vector = fill_struct_no_neu(my_data_2, data_2_intensity_final,structure_vector,  images_names)\n",
    "      structure_feature_vector = np.asarray(structure_feature_vector)\n",
    "      my_df = pd.DataFrame(structure_feature_vector)\n",
    "      my_df.to_csv(structure_features_path,header = False, index= False)\n",
    "      my_data_forPCAss = pd.read_csv(structure_features_path,header = None)  \n",
    "      structure, pac,scaler = doPCA(my_data_forPCAss,pca_comp)\n",
    "      \n",
    "      self.structure_pca = pac\n",
    "      self.structure_scaler = scaler  \n",
    "      return structure\n",
    "\n",
    "\n",
    "    def _get_training_cartoon_and_texture_feature(self, cropped_img_path, cartoon_folder_path, texture_folder_path, cartoon_save_path, texture_save_path,cartoon_pca_comp = 100, texture_pca_comp = 350):\n",
    "      subjects_images = np.sort(os.listdir(self.aligned_training_images_path))      \n",
    "      #cartoon_texture_decomposition(self.aligned_training_images_path, subjects_images, cartoon_folder_path, texture_folder_path)\n",
    "      \n",
    "      path = cartoon_folder_path\n",
    "      cartoon_images = []        \n",
    "      for i in np.sort(os.listdir(cartoon_folder_path)):\n",
    "          cartoon_images.append( path +'/'+ i)         \n",
    "      texture_images = []\n",
    "      path = texture_folder_path\n",
    "      for i in np.sort(os.listdir(texture_folder_path)):\n",
    "          texture_images.append( path + '/' +i)   \n",
    "          \n",
    "      imcart1 = postcartoon(cartoon_images)\n",
    "      imcartoon1 = diffpostcartoon(imcart1, cartoon_images)\n",
    "      print(len(imcartoon1))  \n",
    "\n",
    "      np.savetxt(cartoon_save_path, imcartoon1, delimiter=\",\")\n",
    "\n",
    "      df = struct_read(cartoon_save_path,100)  \n",
    "\n",
    "      cartoon, pac, scaler = doPCA(df, cartoon_pca_comp)\n",
    "      self.cartoon_pca = pac\n",
    "      self.cartoon_scaler = scaler\n",
    "        \n",
    "      imtext1 = posttexture(texture_images)\n",
    "        \n",
    "      np.savetxt(texture_save_path, imtext1, delimiter=\",\")\n",
    "\n",
    "      df = struct_read(texture_save_path,100)  \n",
    "\n",
    "      texture, pac, scaler = doPCA(df, texture_pca_comp)\n",
    "      self.texture_pca = pac  \n",
    "      self.texture_scaler = scaler  \n",
    "    \n",
    "      return cartoon, texture  \n",
    "\n",
    "    def _get_training_feature_vector(self,structure_features_path,cropped_img_path,cartoon_folder_path,texture_folder_path,cartoon_save_path, texture_save_path, shape_pca_comp = 29, struct_pca_comp = 13, cartoon_pca_comp = 100, texture_pca_comp = 350):\n",
    "      shape = self._get_training_shape_feature(shape_pca_comp)\n",
    "      struct = self._get_training_structure_feature(structure_features_path, struct_pca_comp)\n",
    "      cartoon, texture = self._get_training_cartoon_and_texture_feature(cropped_img_path, cartoon_folder_path, texture_folder_path, cartoon_save_path, texture_save_path, cartoon_pca_comp, texture_pca_comp)\n",
    "      b_vector = np.concatenate((shape, struct), axis = 1)\n",
    "      b_vector = np.concatenate((b_vector, cartoon), axis = 1)\n",
    "      b_vector = np.concatenate((b_vector, texture), axis = 1)\n",
    "      self.training_set_features = b_vector\n",
    "      return b_vector  \n",
    "\n",
    "\n",
    "    def _get_testing_shape_feature(self, aligned_testing_images_path, aligned_testing_landmarks_path, shape_test_diff_path, testing_set_img_folder,testing_set_landmarks_folder):\n",
    "      images = np.sort(np.asarray(testing_set_img_folder))\n",
    "      landmarks = np.sort(np.asarray(testing_set_landmarks_folder))\n",
    "      frames_lmarks_XYord = get_landmarks_of_folder(landmarks)\n",
    "      my_df = pd.DataFrame(frames_lmarks_XYord)\n",
    "      intercalated_data = intercalate_data_of_datavectors(my_df, 0)\n",
    "      my_dfc = pd.DataFrame(intercalated_data)\n",
    "\n",
    "      my_landmarks = my_dfc\n",
    "      pathi = aligned_testing_images_path\n",
    "      self.aligned_test_images_path = aligned_testing_images_path \n",
    "      transformed_landmarks_total = align_and_transform_datavector_of_images(images[0], my_landmarks.iloc[0], my_landmarks, images, pathi)\n",
    "\n",
    "      transformed_final = transformed_landmarks_total[0].flatten('F')\n",
    "      for i in range (1, len(transformed_landmarks_total)):\n",
    "          transformed_final = np.vstack((transformed_final, transformed_landmarks_total[i].flatten('F')))\n",
    "\n",
    "      my_df = pd.DataFrame(transformed_final)\n",
    "      my_df.to_csv(aligned_testing_landmarks_path,header = False, index= False)\n",
    "      self.aligned_test_landmarks_path = aligned_testing_landmarks_path\n",
    "      \n",
    "      ind = 0\n",
    "      my_data_aligned_diff = []\n",
    "      images_names = images\n",
    "      current_neutral_index = 0\n",
    "      for i in range(0, len(images_names)):       \n",
    "        if int(images_names[i].split('_')[2].split('.')[0]) == 1:\n",
    "          current_neutral_index = i\n",
    "          \n",
    "        my_data_aligned_diff.append(my_df.iloc[i]- my_df.iloc[current_neutral_index])\n",
    "      my_data_aligned_diff = np.asarray(my_data_aligned_diff)\n",
    "\n",
    "      my_data_forPCA = pd.DataFrame(my_data_aligned_diff)\n",
    "      my_data_forPCA.to_csv(shape_test_diff_path,header = False, index= False)\n",
    "      self.shape_diff_path_test = shape_test_diff_path\n",
    "        \n",
    "      shape = transformPCA(my_data_forPCA, self.shape_pca, self.shape_scaler)\n",
    "   \n",
    "      return shape\n",
    "\n",
    "    def _get_testing_structure_feature(self, structure_features_path_test):\n",
    "      my_data = pd.read_csv(self.aligned_test_landmarks_path,header = None)\n",
    "      subjects_images = self.aligned_test_images_path      \n",
    "      first_frames = get_first_frames(subjects_images,my_data)\n",
    "      first_images = get_first_images(subjects_images,my_data)\n",
    "      intercalated_first_frames = change_to_intercalate_order(first_frames)\n",
    "      structure_vector = do_the_structure_points_and_stack(intercalated_first_frames, first_images)\n",
    "      my_data_2 = pd.read_csv(self.shape_diff_path_test,header = None)  \n",
    "      data_2_intensity_final = get_inten_emo(my_data_2)  \n",
    "      images_names = np.sort(os.listdir(self.aligned_test_images_path))\n",
    "        \n",
    "      structure_feature_vector = fill_struct_no_neu(my_data_2, data_2_intensity_final,structure_vector,  images_names)\n",
    "      structure_feature_vector = np.asarray(structure_feature_vector)\n",
    "      my_df = pd.DataFrame(structure_feature_vector)\n",
    "      my_df.to_csv(structure_features_path_test,header = False, index= False)\n",
    "      my_data_forPCAss = pd.read_csv(structure_features_path_test,header = None) \n",
    "\n",
    "      structure = transformPCA(my_data_forPCAss, self.structure_pca, self.structre_scaler)     \n",
    "    \n",
    "      return structure\n",
    "\n",
    "\n",
    "    def _get_testing_cartoon_and_texture_feature(self, cropped_img_path_test, cartoon_folder_path_test, texture_folder_path_test, cartoon_save_path_test, texture_save_path_test):\n",
    "      subjects_images = np.sort(os.listdir(self.aligned_test_images_path))      \n",
    "      print(self.aligned_test_images_path)  \n",
    "      #cartoon_texture_decomposition(self.aligned_test_images_path, subjects_images, cartoon_folder_path_test, texture_folder_path_test)\n",
    "      \n",
    "      path = cartoon_folder_path_test\n",
    "      cartoon_images = []\n",
    "      for i in np.sort(os.listdir(cartoon_folder_path_test)):\n",
    "        cartoon_images.append( path +'/'+ i)         \n",
    "      texture_images = []\n",
    "      path = texture_folder_path_test\n",
    "      for i in np.sort(os.listdir(texture_folder_path_test)):\n",
    "        texture_images.append( path + '/' +i)   \n",
    "      print(len(cartoon_images))\n",
    "      imcart1 = postcartoon(cartoon_images)\n",
    "      print(len(imcart1))\n",
    "      imcartoon1 = diffpostcartoon(imcart1, cartoon_images)\n",
    "      print(len(imcartoon1))  \n",
    "\n",
    "      np.savetxt(cartoon_save_path_test, imcartoon1, delimiter=\",\")\n",
    "\n",
    "      df = struct_read(cartoon_save_path_test,100)\n",
    "\n",
    "      cartoon = transformPCA(df, self.cartoon_pca, self.cartoon_scaler)      \n",
    "        \n",
    "      imtext1 = posttexture(texture_images)\n",
    "        \n",
    "      np.savetxt(texture_save_path_test, imtext1, delimiter=\",\")\n",
    "\n",
    "      df = struct_read(texture_save_path_test,100)  \n",
    "      \n",
    "      texture = transformPCA(df, self.texture_pca, self.texture_scaler)      \n",
    "        \n",
    "      return cartoon, texture  \n",
    "\n",
    "    def _get_testing_feature_vector(self,aligned_testing_images_path, aligned_testing_landmarks_path, shape_test_diff_path, testing_set_img_folder,testing_set_landmarks_folder,  structure_features_path,cropped_img_path,cartoon_folder_path,texture_folder_path,cartoon_save_path, texture_save_path):\n",
    "      shape = self._get_testing_shape_feature( aligned_testing_images_path, aligned_testing_landmarks_path, shape_test_diff_path, testing_set_img_folder,testing_set_landmarks_folder)\n",
    "      struct = self._get_testing_structure_feature(structure_features_path)\n",
    "      cartoon, texture = self._get_testing_cartoon_and_texture_feature(cropped_img_path, cartoon_folder_path, texture_folder_path, cartoon_save_path, texture_save_path)\n",
    "      b_vector_test = np.concatenate((shape, struct), axis = 1)\n",
    "      b_vector_test = np.concatenate((b_vector_test, cartoon), axis = 1)\n",
    "      b_vector_test = np.concatenate((b_vector_test, texture), axis = 1)\n",
    "      self.testing_set_features = b_vector_test\n",
    "      return b_vector_test\n",
    "        \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e54907-aec4-4ee3-be23-924ff0f3c3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ce7c2-dfc9-407e-a175-f22f703baa90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8794ace7-37e7-4c68-8f9b-9975ae0f31a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e9511-d13f-412c-8073-0ee973a7bd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bd8f2-437c-4d8d-94e1-e9c406c0a136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b4ce8f-6d07-4efb-81d9-14731ecd188a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab1034-87f1-43cf-ae6c-ac403fdd9854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0542b-ff49-4f83-8358-beeb7cbae127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566e026-b99b-43e9-85b9-0665785bd6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5558c-5cd9-41b7-8c92-7bea60fa6913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f5bb9-d6de-41b0-b113-4e01341fe48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a88f8a-029b-4295-b789-54df57274f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3204e33d-1ba5-4bd1-bb1b-53446eedf3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b822177-716f-4521-8ec2-1c4eb7ecfc88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3537f7-02ff-42ef-a67d-5ea5a97905ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e306354-493c-4162-9939-105659cf6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6ea2360e-1382-4eed-aebd-a85d017fbc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53db69-200b-4402-8805-4129d19d94b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "91117254-12ab-49d2-bf06-ac27ddaa9e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e01338-76a4-4557-91e0-1ed26a022f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cffc3c-0fe0-429e-bd20-e551da159b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292ff8f-0f86-4881-8854-19ba3055078d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cca1d4-9d6c-418e-a729-951787045afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8cfb8-7ebd-4b1f-96e2-597d1f4c6e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34102888-1c75-4d55-9c92-86379f36d677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64595b-9065-48d1-a8bc-0a58ef057979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgEnv-kernel",
   "language": "python",
   "name": "imgenv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
