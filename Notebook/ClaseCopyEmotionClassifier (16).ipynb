{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beca2eb3-8614-451a-993d-d7133590e41d",
      "metadata": {
        "id": "beca2eb3-8614-451a-993d-d7133590e41d"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np # NUMPY\n",
        "import pandas as pd # PANDAS\n",
        "import matplotlib.pyplot as plt # MATPLOTLIB\n",
        "import cv2\n",
        "import math\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gs9nQZcv5MSo"
      },
      "id": "Gs9nQZcv5MSo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "752813c5-52a9-4b54-9726-7d6fc0272ac1",
      "metadata": {
        "id": "752813c5-52a9-4b54-9726-7d6fc0272ac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2921a563-4e21-4ed8-ba0c-828fb391a780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn-som\n",
            "  Downloading sklearn_som-1.1.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sklearn-som) (1.26.4)\n",
            "Downloading sklearn_som-1.1.0-py3-none-any.whl (6.7 kB)\n",
            "Installing collected packages: sklearn-som\n",
            "Successfully installed sklearn-som-1.1.0\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "!git clone https://github.com/aliciaamaliaalvarezmon/SynthesisEmotions.git\n",
        "%run /content/SynthesisEmotions/Notebook/auxfunctions.ipynb\n",
        "%run /content/SynthesisEmotions/Notebook/ExpressionMapFunctionsAux.ipynb\n",
        "%run /content/SynthesisEmotions/Notebook/SOM2ver.ipynb\n",
        "#import subprocess\n",
        "#!chmod a+x '/content/SynthesisEmotions/cartoonTexture'\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ebccb8c-b96d-491f-a9b5-8ad67dbf727b",
      "metadata": {
        "id": "4ebccb8c-b96d-491f-a9b5-8ad67dbf727b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c812d60a-54b8-4069-993a-277210c2c593",
      "metadata": {
        "id": "c812d60a-54b8-4069-993a-277210c2c593"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06f213d5-cb1e-4906-b8f9-c3ef2c03b45a",
      "metadata": {
        "id": "06f213d5-cb1e-4906-b8f9-c3ef2c03b45a"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import pickle as pk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f43a5e2-05e3-4611-b335-a9145808db42",
      "metadata": {
        "id": "2f43a5e2-05e3-4611-b335-a9145808db42"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e93bc11-e447-49cd-b3e5-ebab2716d562",
      "metadata": {
        "id": "2e93bc11-e447-49cd-b3e5-ebab2716d562"
      },
      "outputs": [],
      "source": [
        "\n",
        "def doPCA(data, comp):\n",
        "    std_scaler = StandardScaler()\n",
        "    scaler = std_scaler.fit(data)\n",
        "    scaled_landDiff = scaler.transform(data)\n",
        "    pca = PCA(n_components=comp)\n",
        "    #pca.fit_transform(scaled_landDiff)\n",
        "    b_shape = pca.fit_transform(scaled_landDiff)\n",
        "    print(sum(pca.explained_variance_ratio_))\n",
        "    print(b_shape.shape)\n",
        "    return b_shape, pca, scaler\n",
        "\n",
        "def transformPCA(data, pca, scaler):\n",
        "    scaled_landDiff = scaler.transform(data)\n",
        "    return pca.transform(scaled_landDiff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4aa800b-a256-410e-86fd-815751356820",
      "metadata": {
        "id": "f4aa800b-a256-410e-86fd-815751356820"
      },
      "outputs": [],
      "source": [
        "##Vamos a necesitar esto para los V1,V2,V3 anticuados. Usarlo y luego dejarlo comentado\n",
        "def discard_all_extra_neutrals(data, path_folder):\n",
        "  data = np.asarray(data)\n",
        "  image_names = np.sort(os.listdir(path_folder))\n",
        "  current_neutral_index = -1\n",
        "  my_data_for_PCA_SNeu = []\n",
        "  current_subject = 'start'\n",
        "\n",
        "  for i in range(0, len(images_names)):\n",
        "    if int(images_names[i].split('_')[5].split('.')[0]) == 1:\n",
        "      if current_subject != images_names[i].split('_')[3]:\n",
        "        current_subject = images_names[i].split('_')[3]\n",
        "        my_data_for_PCA_SNeu.append(data[i])\n",
        "    else:\n",
        "        my_data_for_PCA_SNeu.append(data[i])\n",
        "\n",
        "  my_data_for_PCA_SNeu = np.asarray(my_data_for_PCA_SNeu)\n",
        "  return my_data_for_PCA_SNeu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "gppi8PmVXjz2"
      },
      "id": "gppi8PmVXjz2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_first_frames_and_images(path,my_data):\n",
        "    first_images = []\n",
        "    first_frames = []\n",
        "    total = 0\n",
        "\n",
        "    current_neutral_index = 0\n",
        "    images_names = np.sort(os.listdir(path))\n",
        "    for i in range(0, len(images_names)):\n",
        "        print(images_names[i].split('_'))\n",
        "        if int(images_names[i].split('_')[5].split('.')[0]) == 1:\n",
        "            first_frames.append(my_data.iloc[i])\n",
        "            first_images.append(path +'/'+ images_names[i])\n",
        "\n",
        "    first_frames = np.asarray( first_frames, dtype=object)\n",
        "    first_images = np.asarray( first_images, dtype=object)\n",
        "\n",
        "    return first_frames, first_images\n",
        "\n",
        "def get_first_frames_and_images_test(path,my_data):\n",
        "    first_images = []\n",
        "    first_frames = []\n",
        "    total = 0\n",
        "\n",
        "    current_neutral_index = 0\n",
        "    images_names = np.sort(os.listdir(path))\n",
        "    for i in range(0, len(images_names)):\n",
        "      first_frames.append(my_data.iloc[i])\n",
        "      first_images.append(path +'/'+ images_names[i])\n",
        "\n",
        "    first_frames = np.asarray( first_frames, dtype=object)\n",
        "    first_images = np.asarray( first_images, dtype=object)\n",
        "\n",
        "    return first_frames, first_images"
      ],
      "metadata": {
        "id": "hJ2bKGIUX_2n"
      },
      "id": "hJ2bKGIUX_2n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "939c031d-8f01-420b-a983-ca5863cd75c2",
      "metadata": {
        "id": "939c031d-8f01-420b-a983-ca5863cd75c2"
      },
      "outputs": [],
      "source": [
        "def align_and_transform_datavector_of_images(reference_image, reference_landmarks, landmarks, images, pathi):\n",
        "    transformed_landmarks_total = []\n",
        "    for i in range(0, len(landmarks)):\n",
        "        tr_Y_img, Z_pts = align_and_transform_image(reference_image, reference_landmarks, landmarks.iloc[i], images[i])\n",
        "        transformed_landmarks_total.append(Z_pts)\n",
        "        filename = 'warped_image_procustres_'+ str(images[i].split('/')[8])+'.png'\n",
        "        path = pathi\n",
        "        print(os.path.join(path , filename))\n",
        "        cv2.imwrite(os.path.join(path , filename), tr_Y_img)\n",
        "    transformed_landmarks_total = np.asarray(transformed_landmarks_total)\n",
        "    return transformed_landmarks_total\n",
        "\n",
        "def get_inten_emo(my_data_2):\n",
        "  Ai = my_data_2.max()\n",
        "  data_2_intensity = my_data_2\n",
        "  data_2_intensity_div = data_2_intensity/Ai\n",
        "  data_2_intensity_sum =  np.sum(data_2_intensity_div, axis=1)\n",
        "  data_2_intensity_final = data_2_intensity_sum/136\n",
        "  return data_2_intensity_final"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "4po1nLlWt2Ho"
      },
      "id": "4po1nLlWt2Ho",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1486c7-e4d6-4f49-81e3-b5bbf2d61121",
      "metadata": {
        "id": "ab1486c7-e4d6-4f49-81e3-b5bbf2d61121"
      },
      "outputs": [],
      "source": [
        "def fill_struct_no_neu(my_data_2, data_2_intensity_final, structure_vector, no_neutral_im):\n",
        "    cant_features = 22\n",
        "    current_sub = 'start'\n",
        "    ab = 0\n",
        "    structure_feature_vector = np.zeros((len(my_data_2), cant_features))\n",
        "    current_features_index = -1\n",
        "    for i in range(0, len(data_2_intensity_final)):\n",
        "        print('fill')\n",
        "        print(no_neutral_im[ab].split('_'))\n",
        "        if  no_neutral_im[ab].split('_')[3] != current_sub:\n",
        "            current_sub = no_neutral_im[ab].split('_')[3]\n",
        "            current_features_index = current_features_index + 1\n",
        "        vector = structure_vector[:,current_features_index]#columna i\n",
        "        vector = np.append(vector,data_2_intensity_final.iloc[i])\n",
        "        structure_feature_vector[ab] = vector\n",
        "        ab = ab + 1\n",
        "    return structure_feature_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a95207-66a4-479a-9bc3-24de09819a80",
      "metadata": {
        "id": "c1a95207-66a4-479a-9bc3-24de09819a80"
      },
      "outputs": [],
      "source": [
        "def cartoon_texture_decomposition(images_path, subject_images, cartoon_folder_path, texture_folder_path,  forShow='True', j=0.2):\n",
        "    for i in range (0,  len(subject_images)):\n",
        "        subject = subject_images[i].split('/')[0].split('_')[3]\n",
        "        serie = subject_images[i].split('/')[0].split('_')[4]\n",
        "        frame = subject_images[i].split('/')[0].split('_')[5].split('.')[0]\n",
        "        print(subject,serie, frame)\n",
        "        image_path = images_path + '/' + subject_images[i]\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        img2 = img[40:450, 170:580]\n",
        "        cv2.imwrite(cropped_img_path, img2)\n",
        "        cartoon_path = cartoon_folder_path + \"/cartoonimg\"+'_'+ str(int(j*100))+'_' + subject +'_' +serie +'_'+frame +\".png\"\n",
        "        texture_path = texture_folder_path +\"/textureimg\"+'_'+ str(int(j*100))+'_' + subject +'_' +serie +'_'+frame +\".png\"\n",
        "        if(forShow == 'True'):\n",
        "          subprocess.check_output(['/content/SynthesisEmotions/cartoonTexture',cropped_img_path, str(j), cartoon_path, texture_path])\n",
        "        else:\n",
        "          subprocess.check_output(['/content/cartoonTextureNew',cropped_img_path, str(j), cartoon_path, texture_path])\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d114241d-11d6-4783-baf9-23c9e6852adc",
      "metadata": {
        "id": "d114241d-11d6-4783-baf9-23c9e6852adc"
      },
      "outputs": [],
      "source": [
        "def postcartoon(cartimlist):\n",
        "  imcart = []\n",
        "  for i in cartimlist:\n",
        "    img = cv2.imread(i, cv2.IMREAD_GRAYSCALE)\n",
        "    img_mean = img/np.mean(img)\n",
        "    imcart.append(img_mean.flatten())\n",
        "  return imcart\n",
        "\n",
        "def diffpostcartoon(cartarray, cartimlist):\n",
        "  imcartoon = []\n",
        "  index = -1\n",
        "  subject = 'start'\n",
        "  current_image = cartarray[0]\n",
        "  for i in range(0, len(cartimlist)):\n",
        "    print(cartimlist[i].split('/'))\n",
        "    if (int(cartimlist[i].split('/')[3].split('_')[4].split('.')[0]) == 1):\n",
        "      index = i\n",
        "      current_image = cartarray[i]\n",
        "    imcartoon.append(cartarray[i]- current_image)\n",
        "  return imcartoon\n",
        "\n",
        "def eliminate_extraneutrals(cartimlist, cartoondiff):\n",
        "  no_neutral_extra_im = []\n",
        "  cheking_order = []\n",
        "  current_sub = 'start'\n",
        "  for i in range (0, len(cartimlist)):\n",
        "    if cartimlist[i].split('_')[3] != current_sub:\n",
        "      current_sub = cartimlist[i].split('_')[3]\n",
        "      no_neutral_extra_im.append(cartoondiff[i])\n",
        "      cheking_order.append(cartimlist[i])\n",
        "    if int(cartimlist[i].split('_')[5].split('.')[0]) != 1:\n",
        "      no_neutral_extra_im.append(cartoondiff[i])\n",
        "      cheking_order.append(cartimlist[i])\n",
        "  no_neutral_extra_im = np.asarray(no_neutral_extra_im)\n",
        "  no_neutral_extra_im.shape\n",
        "  return no_neutral_extra_im, cheking_order\n",
        "\n",
        "def posttexture(teximlist):\n",
        "  imtext = []\n",
        "  for i in teximlist:\n",
        "    img = cv2.imread(i, cv2.IMREAD_GRAYSCALE)\n",
        "    imtext.append(img.flatten())\n",
        "\n",
        "  imtextarray = np.asarray(imtext)\n",
        "  imtextarray.shape\n",
        "  return imtextarray\n",
        "\n",
        "def struct_read(path, chunk_size = 100):\n",
        "  file_path = path\n",
        "  df_list = []\n",
        "  for chunk in pd.read_csv(file_path, chunksize=chunk_size, header = None):\n",
        "    df_list.append(chunk)\n",
        "  df = pd.concat(df_list)\n",
        "  return df  #esto equivale a tirar por balde el ahorro. creo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "613ebcd6-a0c5-4320-b80e-e029ba1424bb",
      "metadata": {
        "id": "613ebcd6-a0c5-4320-b80e-e029ba1424bb"
      },
      "outputs": [],
      "source": [
        "class EmotionClassifier():\n",
        "    \"\"\"\n",
        "    The 2-D, rectangular grid self-organizing map class using Numpy.\n",
        "    \"\"\"\n",
        "    def __init__(self, training_data_images, training_data_landmarks, aligned_training_images_path, aligned_training_landmarks_path, shape_diff_path,  m=9, n=9, lr=0.25, sigma=1.75, dim = 392):\n",
        "\n",
        "\n",
        "        # Initialize descriptive features of SOM\n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        self.shape = (m, n)\n",
        "        self.lr = lr\n",
        "        self.sigma = sigma\n",
        "        self.dim = dim\n",
        "\n",
        "\n",
        "\n",
        "        self.training_set_img_folder = training_data_images\n",
        "        self.training_set_landmarks_folder = training_data_landmarks\n",
        "\n",
        "        self.aligned_training_images_path = aligned_training_images_path\n",
        "        self.aligned_training_landmarks_path = aligned_training_landmarks_path\n",
        "        self.shape_diff_path = shape_diff_path\n",
        "        self.aligned_test_images_path = None\n",
        "        self.aligned_test_landmarks_path = None\n",
        "        self.shape_diff_path_test = None\n",
        "\n",
        "        # Initialize weights\n",
        "\n",
        "        self.ref_img = None\n",
        "        self.ref_landmarks = None\n",
        "\n",
        "        self.shape_pca = None\n",
        "        self.structure_pca = None\n",
        "        self.cartoon_pca = None\n",
        "        self.texture_pca = None\n",
        "\n",
        "        self.shape_scaler = None\n",
        "        self.structure_scaler = None\n",
        "        self.cartoon_scaler = None\n",
        "        self.texture_scaler = None\n",
        "\n",
        "        self.training_set_features = None\n",
        "        self.test_set_features = None\n",
        "\n",
        "        self.som = None\n",
        "\n",
        "\n",
        "\n",
        "        # Set after fitting\n",
        "\n",
        "\n",
        "    def _change_training_img_loc(self, loc):\n",
        "        \"\"\"\n",
        "        Return the indices of an m by n array.\n",
        "        \"\"\"\n",
        "        self.training_set_img_folder = loc\n",
        "        return self.training_set_img_folder\n",
        "\n",
        "    def _change_test_img_loc(self, loc):\n",
        "        \"\"\"\n",
        "        Return the indices of an m by n array.\n",
        "        \"\"\"\n",
        "        self.test_set_img_folder = loc\n",
        "        return self.test_set_img_folder\n",
        "\n",
        "    def _change_training_lan_loc(self, loc):\n",
        "        \"\"\"\n",
        "        Return the indices of an m by n array.\n",
        "        \"\"\"\n",
        "        self.training_set_landmarks_folder = loc\n",
        "        return self.training_set_landmarks_folder\n",
        "\n",
        "    def _change_test_lan_loc(self, loc):\n",
        "        \"\"\"\n",
        "        Return the indices of an m by n array.\n",
        "        \"\"\"\n",
        "        self.test_set_landmarks_folder = loc\n",
        "        return self.test_set_landmarks_folder\n",
        "\n",
        "    def _save_shape_pca(self):\n",
        "      pk.dump(self.shape_pca, open(\"shape_pca.pkl\",\"wb\"))\n",
        "\n",
        "    def _load_shape_pca(self):\n",
        "      self.shape_pca = pk.load(open(\"shape_pca.pkl\",\"rb\"))\n",
        "      return self.shape_pca\n",
        "\n",
        "    def _save_shape_scaler(self):\n",
        "      pk.dump(self.shape_scaler, open(\"shape_scaler.pkl\",\"wb\"))\n",
        "\n",
        "    def _load_shape_scaler(self):\n",
        "      self.shape_scaler = pk.load(open(\"shape_scaler.pkl\",\"rb\"))\n",
        "      return self.shape_scaler\n",
        "\n",
        "    def _save_structure_pca(self):\n",
        "      pk.dump(self.structure_pca, open(\"structure_pca.pkl\",\"wb\"))\n",
        "\n",
        "    def _load_structure_pca(self):\n",
        "      self.strucure_pca = pk.load(open(\"structure_pca.pkl\",\"rb\"))\n",
        "      return self.strucure_pca\n",
        "\n",
        "    def _save_structure_scaler(self):\n",
        "      pk.dump(self.structure_scaler, open(\"structure_scaler.pkl\",\"wb\"))\n",
        "\n",
        "    def _load_structure_scaler(self):\n",
        "      self.strucure_scaler = pk.load(open(\"structure_scaler.pkl\",\"rb\"))\n",
        "      return self.strucure_scaler\n",
        "\n",
        "    def _save_cartoon_pca(self):\n",
        "      pk.dump(self.cartoon_pca, open(\"cartoon_pca.pkl\",\"wb\"))\n",
        "\n",
        "    def _load_cartoon_pca(self):\n",
        "      self.cartoon_pca = pk.load(open(\"cartoon_pca.pkl\",\"rb\"))\n",
        "      return self.cartoon_pca\n",
        "\n",
        "    def _save_cartoon_scaler(self):\n",
        "      pk.dump(self.cartoon_scaler, open(\"cartoon_scaler.pkl\",\"wb\"))\n",
        "\n",
        "    def _load_cartoon_scaler(self):\n",
        "      self.cartoon_scaler = pk.load(open(\"cartoon_scaler.pkl\",\"rb\"))\n",
        "      return self.cartoon_scaler\n",
        "\n",
        "    def _save_texture_pca(self):\n",
        "      pk.dump(self.texture_pca, open(\"texture_pca.pkl\",\"wb\"))\n",
        "\n",
        "    def _load_texture_pca(self):\n",
        "      self.texture_pca = pk.load(open(\"texture_pca.pkl\",\"rb\"))\n",
        "      return self.texture_pca\n",
        "\n",
        "    def _save_texture_scaler(self):\n",
        "      pk.dump(self.texture_scaler, open(\"texture_scaler.pkl\",\"wb\"))\n",
        "\n",
        "    def _load_texture_scaler(self):\n",
        "      self.texture_scaler = pk.load(open(\"texture_scaler.pkl\",\"rb\"))\n",
        "      return self.texture_scaler\n",
        "\n",
        "\n",
        "    def _get_training_shape_feature(self, pca_comp = 29):\n",
        "\n",
        "      images = np.sort(np.asarray(self.training_set_img_folder))\n",
        "      landmarks = np.sort(np.asarray(self.training_set_landmarks_folder))\n",
        "      frames_lmarks_XYord = get_landmarks_of_folder(landmarks)\n",
        "      my_df = pd.DataFrame(frames_lmarks_XYord)\n",
        "      intercalated_data = intercalate_data_of_datavectors(my_df, 0)\n",
        "      my_dfc = pd.DataFrame(intercalated_data)\n",
        "\n",
        "      my_landmarks = my_dfc\n",
        "      pathi = self.aligned_training_images_path\n",
        "\n",
        "      self.ref_img = images[0]\n",
        "      self.ref_landmarks = my_landmarks.iloc[0]\n",
        "      transformed_landmarks_total = align_and_transform_datavector_of_images(images[0], my_landmarks.iloc[0], my_landmarks, images, pathi)\n",
        "\n",
        "      transformed_final = transformed_landmarks_total[0].flatten('F')\n",
        "      for i in range (1, len(transformed_landmarks_total)):\n",
        "        transformed_final = np.vstack((transformed_final, transformed_landmarks_total[i].flatten('F')))\n",
        "\n",
        "      my_df = pd.DataFrame(transformed_final)\n",
        "      my_df.to_csv(self.aligned_training_landmarks_path,header = False, index= False)\n",
        "\n",
        "      ind = 0\n",
        "      my_data_aligned_diff = []\n",
        "      images_names = images\n",
        "      current_neutral_index = 0\n",
        "      for i in range(0, len(images_names)):\n",
        "        if int(images_names[i].split('_')[2].split('.')[0]) == 1:\n",
        "          current_neutral_index = i\n",
        "          #print('cambie neutral '+ str(current_neutral_index))\n",
        "        my_data_aligned_diff.append(my_df.iloc[i]- my_df.iloc[current_neutral_index])\n",
        "      my_data_aligned_diff = np.asarray(my_data_aligned_diff)\n",
        "\n",
        "      my_data_forPCA = pd.DataFrame(my_data_aligned_diff)\n",
        "      my_data_forPCA.to_csv(self.shape_diff_path,header = False, index= False)\n",
        "\n",
        "      shape,pac, scaler= doPCA(my_data_forPCA, pca_comp)\n",
        "\n",
        "      self.shape_pca = pac\n",
        "      self.shape_scaler = scaler\n",
        "      return shape\n",
        "\n",
        "    def _get_training_structure_feature(self, structure_features_path, pca_comp = 13):\n",
        "      my_data = pd.read_csv(self.aligned_training_landmarks_path,header = None)\n",
        "      subjects_images = self.aligned_training_images_path\n",
        "      first_frames = get_first_frames(subjects_images,my_data)\n",
        "      first_images = get_first_images(subjects_images,my_data)\n",
        "      intercalated_first_frames = change_to_intercalate_order(first_frames)\n",
        "      structure_vector = do_the_structure_points_and_stack(intercalated_first_frames, first_images)\n",
        "      my_data_2 = pd.read_csv(self.shape_diff_path,header = None)\n",
        "      data_2_intensity_final = get_inten_emo(my_data_2)\n",
        "      images_names = np.sort(os.listdir(self.aligned_training_images_path))\n",
        "\n",
        "      structure_feature_vector = fill_struct_no_neu(my_data_2, data_2_intensity_final,structure_vector,  images_names)\n",
        "      structure_feature_vector = np.asarray(structure_feature_vector)\n",
        "      my_df = pd.DataFrame(structure_feature_vector)\n",
        "      my_df.to_csv(structure_features_path,header = False, index= False)\n",
        "      my_data_forPCAss = pd.read_csv(structure_features_path,header = None)\n",
        "      structure, pac,scaler = doPCA(my_data_forPCAss,pca_comp)\n",
        "\n",
        "      self.structure_pca = pac\n",
        "      self.structure_scaler = scaler\n",
        "      return structure\n",
        "\n",
        "\n",
        "    def _get_training_cartoon_and_texture_feature(self, cropped_img_path, cartoon_folder_path, texture_folder_path, cartoon_save_path, texture_save_path,make ='True',  forShow='True', cartoon_pca_comp = 100, texture_pca_comp = 250, j =0.2):\n",
        "      subjects_images = np.sort(os.listdir(self.aligned_training_images_path))\n",
        "      if(make == 'True'):\n",
        "        cartoon_texture_decomposition(self.aligned_training_images_path, subjects_images, cartoon_folder_path, texture_folder_path, forShow, j)\n",
        "\n",
        "      path = cartoon_folder_path\n",
        "      cartoon_images = []\n",
        "      for i in np.sort(os.listdir(cartoon_folder_path)):\n",
        "          cartoon_images.append( path +'/'+ i)\n",
        "      texture_images = []\n",
        "      path = texture_folder_path\n",
        "      for i in np.sort(os.listdir(texture_folder_path)):\n",
        "          texture_images.append( path + '/' +i)\n",
        "\n",
        "      imcart1 = postcartoon(cartoon_images)\n",
        "      imcartoon1 = diffpostcartoon(imcart1, cartoon_images)\n",
        "      print(len(imcartoon1))\n",
        "\n",
        "      np.savetxt(cartoon_save_path, imcartoon1, delimiter=\",\")\n",
        "\n",
        "      df = struct_read(cartoon_save_path,100)\n",
        "\n",
        "      cartoon, pac, scaler = doPCA(df, cartoon_pca_comp)\n",
        "      self.cartoon_pca = pac\n",
        "      self.cartoon_scaler = scaler\n",
        "\n",
        "      imtext1 = posttexture(texture_images)\n",
        "\n",
        "      np.savetxt(texture_save_path, imtext1, delimiter=\",\")\n",
        "\n",
        "      df = struct_read(texture_save_path,100)\n",
        "\n",
        "      texture, pac, scaler = doPCA(df, texture_pca_comp)\n",
        "      self.texture_pca = pac\n",
        "      self.texture_scaler = scaler\n",
        "\n",
        "      return cartoon, texture\n",
        "\n",
        "    def _get_training_feature_vector(self,structure_features_path,cropped_img_path,cartoon_folder_path,texture_folder_path,cartoon_save_path, texture_save_path, shape_pca_comp = 29, struct_pca_comp = 13, cartoon_pca_comp = 100, texture_pca_comp = 350):\n",
        "      shape = self._get_training_shape_feature(shape_pca_comp)\n",
        "      struct = self._get_training_structure_feature(structure_features_path, struct_pca_comp)\n",
        "      cartoon, texture = self._get_training_cartoon_and_texture_feature(cropped_img_path, cartoon_folder_path, texture_folder_path, cartoon_save_path, texture_save_path, cartoon_pca_comp, texture_pca_comp)\n",
        "      b_vector = np.concatenate((shape, struct), axis = 1)\n",
        "      b_vector = np.concatenate((b_vector, cartoon), axis = 1)\n",
        "      b_vector = np.concatenate((b_vector, texture), axis = 1)\n",
        "      self.training_set_features = b_vector\n",
        "      return b_vector\n",
        "\n",
        "\n",
        "    def _get_testing_shape_feature(self,  testing_set_img_folder,testing_set_landmarks_folder,aligned_testing_images_path, aligned_testing_landmarks_path, shape_test_diff_path):\n",
        "      #_get_testing_shape_feature(testing_img_set, testing_lab_set, alig_im_test, alig_lan_test, diff_path_test)\n",
        "      print('llegue')\n",
        "      print(np.sort(np.asarray(testing_set_img_folder)))\n",
        "      images = np.sort(np.asarray(testing_set_img_folder))\n",
        "      landmarks = np.sort(np.asarray(testing_set_landmarks_folder))\n",
        "      print('llegue')\n",
        "      frames_lmarks_XYord = get_landmarks_of_folder(landmarks)\n",
        "      print('llegue 2')\n",
        "      my_df = pd.DataFrame(frames_lmarks_XYord)\n",
        "      intercalated_data = intercalate_data_of_datavectors(my_df, 0)\n",
        "      print('llegue 3')\n",
        "      my_dfc = pd.DataFrame(intercalated_data)\n",
        "\n",
        "      my_landmarks = my_dfc\n",
        "      pathi = aligned_testing_images_path\n",
        "      self.aligned_test_images_path = aligned_testing_images_path\n",
        "      # transformed_landmarks_total = align_and_transform_datavector_of_images(images[0], my_landmarks.iloc[0], my_landmarks, images, pathi)\n",
        "\n",
        "      transformed_landmarks_total = align_and_transform_datavector_of_images(self.ref_img, self.ref_landmarks, my_landmarks, images, pathi)\n",
        "      print('llegue 4')\n",
        "      transformed_final = transformed_landmarks_total[0].flatten('F')\n",
        "      for i in range (1, len(transformed_landmarks_total)):\n",
        "          transformed_final = np.vstack((transformed_final, transformed_landmarks_total[i].flatten('F')))\n",
        "      print('llegue 5')\n",
        "      my_df = pd.DataFrame(transformed_final)\n",
        "      my_df.to_csv(aligned_testing_landmarks_path,header = False, index= False)\n",
        "      self.aligned_test_landmarks_path = aligned_testing_landmarks_path\n",
        "\n",
        "      ind = 0\n",
        "      my_data_aligned_diff = []\n",
        "      images_names = images\n",
        "      current_neutral_index = 0\n",
        "      for i in range(0, len(images_names)):\n",
        "        if int(images_names[i].split('_')[2].split('.')[0]) == 1:\n",
        "          current_neutral_index = i\n",
        "\n",
        "        my_data_aligned_diff.append(my_df.iloc[i]- my_df.iloc[current_neutral_index])\n",
        "      my_data_aligned_diff = np.asarray(my_data_aligned_diff)\n",
        "\n",
        "      my_data_forPCA = pd.DataFrame(my_data_aligned_diff)\n",
        "      my_data_forPCA.to_csv(shape_test_diff_path,header = False, index= False)\n",
        "      self.shape_diff_path_test = shape_test_diff_path\n",
        "\n",
        "      shape = transformPCA(my_data_forPCA, self.shape_pca, self.shape_scaler)\n",
        "\n",
        "      return shape\n",
        "\n",
        "    def _get_testing_structure_feature(self, structure_features_path_test):\n",
        "      my_data = pd.read_csv(self.aligned_test_landmarks_path,header = None)\n",
        "      subjects_images = self.aligned_test_images_path\n",
        "      print(self.aligned_test_landmarks_path)\n",
        "      print(self.aligned_test_images_path)\n",
        "      first_frames, first_images =get_first_frames_and_images_test(subjects_images,my_data)\n",
        "      #first_frames = get_first_frames(subjects_images,my_data)\n",
        "      #first_images = get_first_images(subjects_images,my_data)\n",
        "      intercalated_first_frames = change_to_intercalate_order(first_frames)\n",
        "      print(np.asarray(intercalated_first_frames).shape)\n",
        "      print(np.asarray(first_frames).shape)\n",
        "      structure_vector = do_the_structure_points_and_stack(intercalated_first_frames, first_images)\n",
        "      my_data_2 = pd.read_csv(self.shape_diff_path_test,header = None)\n",
        "      data_2_intensity_final = get_inten_emo(my_data_2)\n",
        "      images_names = np.sort(os.listdir(self.aligned_test_images_path))\n",
        "      print(structure_vector.shape)\n",
        "      structure_feature_vector = fill_struct_no_neu(my_data_2, data_2_intensity_final,structure_vector,  images_names)\n",
        "      structure_feature_vector = np.asarray(structure_feature_vector)\n",
        "      my_df = pd.DataFrame(structure_feature_vector)\n",
        "      my_df.to_csv(structure_features_path_test,header = False, index= False)\n",
        "      my_data_forPCAss = pd.read_csv(structure_features_path_test,header = None)\n",
        "\n",
        "      structure = transformPCA(my_data_forPCAss, self.structure_pca, self.structure_scaler)\n",
        "\n",
        "      return structure\n",
        "\n",
        "\n",
        "    def _get_testing_cartoon_and_texture_feature(self, cropped_img_path_test, cartoon_folder_path_test, texture_folder_path_test, cartoon_save_path_test, texture_save_path_test, make = 'True'):\n",
        "      subjects_images = np.sort(os.listdir(self.aligned_test_images_path))\n",
        "      print(self.aligned_test_images_path)\n",
        "      if make == 'True':\n",
        "        cartoon_texture_decomposition(self.aligned_test_images_path, subjects_images, cartoon_folder_path_test, texture_folder_path_test)\n",
        "\n",
        "      path = cartoon_folder_path_test\n",
        "      cartoon_images = []\n",
        "      for i in np.sort(os.listdir(cartoon_folder_path_test)):\n",
        "        cartoon_images.append( path +'/'+ i)\n",
        "      texture_images = []\n",
        "      path = texture_folder_path_test\n",
        "      for i in np.sort(os.listdir(texture_folder_path_test)):\n",
        "        texture_images.append( path + '/' +i)\n",
        "      print(len(cartoon_images))\n",
        "      imcart1 = postcartoon(cartoon_images)\n",
        "      print(len(imcart1))\n",
        "      imcartoon1 = diffpostcartoon(imcart1, cartoon_images)\n",
        "      print(len(imcartoon1))\n",
        "\n",
        "      np.savetxt(cartoon_save_path_test, imcartoon1, delimiter=\",\")\n",
        "\n",
        "      df = struct_read(cartoon_save_path_test,100)\n",
        "\n",
        "      cartoon = transformPCA(df, self.cartoon_pca, self.cartoon_scaler)\n",
        "\n",
        "      imtext1 = posttexture(texture_images)\n",
        "\n",
        "      np.savetxt(texture_save_path_test, imtext1, delimiter=\",\")\n",
        "\n",
        "      df = struct_read(texture_save_path_test,100)\n",
        "\n",
        "      texture = transformPCA(df, self.texture_pca, self.texture_scaler)\n",
        "\n",
        "      return cartoon, texture\n",
        "\n",
        "    def _get_testing_feature_vector(self,aligned_testing_images_path, aligned_testing_landmarks_path, shape_test_diff_path, testing_set_img_folder,testing_set_landmarks_folder,  structure_features_path,cropped_img_path,cartoon_folder_path,texture_folder_path,cartoon_save_path, texture_save_path):\n",
        "      shape = self._get_testing_shape_feature( aligned_testing_images_path, aligned_testing_landmarks_path, shape_test_diff_path, testing_set_img_folder,testing_set_landmarks_folder)\n",
        "      struct = self._get_testing_structure_feature(structure_features_path)\n",
        "      cartoon, texture = self._get_testing_cartoon_and_texture_feature(cropped_img_path, cartoon_folder_path, texture_folder_path, cartoon_save_path, texture_save_path)\n",
        "      b_vector_test = np.concatenate((shape, struct), axis = 1)\n",
        "      b_vector_test = np.concatenate((b_vector_test, cartoon), axis = 1)\n",
        "      b_vector_test = np.concatenate((b_vector_test, texture), axis = 1)\n",
        "      self.testing_set_features = b_vector_test\n",
        "      return b_vector_test\n",
        "\n",
        "    def get_som(self, max_iter = 3000):\n",
        "      self.som = SOM(self.m, self.n, self.dim, self.lr, self.sigma)\n",
        "      return self.som\n",
        "\n",
        "    def fit_som(epo = 1):\n",
        "      return self.som.fit(training_set_features, epo)\n",
        "\n",
        "    def get_labelAndTarget(training_set):\n",
        "      return 0\n",
        "\n",
        "\n",
        "    def draw_som(self, imagepath):\n",
        "      b_vector_label, b_target = get_labelAndTarget(self.training_set_features)\n",
        "      dic_v2 = win_map(self.som, b_vector_label, return_indices=True)\n",
        "      len(dic_v2.keys())\n",
        "      newDic = get_labeledDic(self.som, b_vector_label, b_target)\n",
        "      draw_labeled_neurons_map(self.som, newDic, imagepath,'png')\n",
        "      return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJ1BHs4p3inw"
      },
      "id": "MJ1BHs4p3inw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c64595b-9065-48d1-a8bc-0a58ef057979",
      "metadata": {
        "id": "5c64595b-9065-48d1-a8bc-0a58ef057979"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "from glob import glob\n",
        "\n",
        "path_of_used_series_landmarks = []\n",
        "path_of_used_series_images = []\n",
        "for i in np.sort(glob(\"/content/SynthesisEmotions/ck+copyAgainT/Landmarks/*/*/\", recursive = True)):\n",
        "\n",
        "    subject = i.split('/')[5]\n",
        "    serie = i.split('/')[6]\n",
        "    actual = subject + '/'+ serie\n",
        "    path = '/content/SynthesisEmotions/ck+copyAgainT/Emotion_labels/Emotion/' +actual + '/'\n",
        "    image_path = '/content/SynthesisEmotions/ck+copyAgainT/extended-cohn-kanade-images/cohn-kanade-images/' +actual+ '/'\n",
        "    landmark_path = '/content/SynthesisEmotions/ck+copyAgainT/Landmarks/' + actual+ '/'\n",
        "\n",
        "    filelabel = np.sort((os.listdir(path)))[0]\n",
        "    longitud = np.sort((os.listdir(str(i))))\n",
        "    imagelabel = np.sort(os.listdir(str(image_path)))\n",
        "    landmarklabel = np.sort(os.listdir(str(landmark_path)))\n",
        "    with open(path+ '/'+filelabel) as f:\n",
        "        line = float(f.readline())\n",
        "    if line == 4 or line == 6:\n",
        "        path_of_used_series_landmarks.append(landmark_path + landmarklabel[0])\n",
        "        path_of_used_series_landmarks.append(landmark_path + landmarklabel[len(longitud)-2])\n",
        "        path_of_used_series_landmarks.append(landmark_path + landmarklabel[len(longitud)-1])\n",
        "        path_of_used_series_images.append(image_path + imagelabel[0])\n",
        "        path_of_used_series_images.append(image_path + imagelabel[len(longitud)-2])\n",
        "        path_of_used_series_images.append(image_path + imagelabel[len(longitud)-1])\n",
        "    else:\n",
        "        path_of_used_series_landmarks.append(landmark_path + landmarklabel[0])\n",
        "        path_of_used_series_landmarks.append(landmark_path + landmarklabel[len(longitud)-1])\n",
        "        path_of_used_series_images.append(image_path + imagelabel[0])\n",
        "        path_of_used_series_images.append(image_path + imagelabel[len(longitud)-1])\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "path_of_used_series_landmarks_test = []\n",
        "path_of_used_series_images_test = []\n",
        "for i in np.sort(glob(\"/content/SynthesisEmotions/ck+copyAgain/Landmarks/*/*/\", recursive = True)):\n",
        "    subject = i.split('/')[5]\n",
        "    serie = i.split('/')[6]\n",
        "    actual = subject + '/'+ serie\n",
        "    path = '/content/SynthesisEmotions/ck+copyAgain/Emotion_labels/Emotion/' +actual + '/'\n",
        "    image_path = '/content/SynthesisEmotions/ck+copyAgain/extended-cohn-kanade-images/cohn-kanade-images/' +actual+ '/'\n",
        "    landmark_path = '/content/SynthesisEmotions/ck+copyAgain/Landmarks/' + actual+ '/'\n",
        "    if actual != 'S084/004':\n",
        "      filelabel = np.sort((os.listdir(path)))[0]\n",
        "      longitud = np.sort((os.listdir(str(i))))\n",
        "      imagelabel = np.sort(os.listdir(str(image_path)))\n",
        "      landmarklabel = np.sort(os.listdir(str(landmark_path)))\n",
        "      #path_of_used_series_landmarks_test.append(landmark_path + landmarklabel[0])\n",
        "      path_of_used_series_landmarks_test.append(landmark_path + landmarklabel[len(longitud)-1])\n",
        "      #path_of_used_series_images_test.append(image_path + imagelabel[0])\n",
        "      path_of_used_series_images_test.append(image_path + imagelabel[len(longitud)-1])\n",
        "#S084/04 la saqu√© porque era disgust\n",
        "\n",
        "training_img_set =  path_of_used_series_images\n",
        "training_lab_set = path_of_used_series_landmarks\n",
        "testing_img_set = path_of_used_series_images_test\n",
        "testing_lab_set = path_of_used_series_landmarks_test\n",
        "alig_im= '/content/probando_aligned_img'\n",
        "alig_lan = '/content/probando_aligned_landmarks.csv'\n",
        "diff_path = '/content/probando_shapediff.csv'\n",
        "alig_im_test= '/content/probando_aligned_img_test'\n",
        "alig_lan_test = '/content/probando_aligned_landmarks_test.csv'\n",
        "diff_path_test = '/content/probando_shapediff_test.csv'\n",
        "structure_f_path = '/content/probando_struct_features.csv'\n",
        "structure_f_path_test = '/content/probando_struct_features_test.csv'\n",
        "cartoon_folder_path = '/content/probando_cartoon_folder'\n",
        "texture_folder_path = '/content/probando_texture_folder'\n",
        "cropped_img_path = '/content/cropped_img.png'\n",
        "texture_save_path ='/content/texture_save.csv'\n",
        "cartoon_save_path ='/content/cartoon_save.csv'\n",
        "cartoon_folder_path_test = '/content/probando_cartoon_folder_test'\n",
        "texture_folder_path_test = '/content/probando_texture_folder_test'\n",
        "cropped_img_path_test = '/content/cropped_img_test.png'\n",
        "texture_save_path_test ='/content/texture_save_test.csv'\n",
        "cartoon_save_path_test ='/content/cartoon_save_test.csv'\n",
        "\n",
        "classi = EmotionClassifier(training_img_set, training_lab_set, alig_im, alig_lan, diff_path)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(np.sort(np.asarray(testing_img_set)).shape)\n"
      ],
      "metadata": {
        "id": "xFuExNWd8FLA"
      },
      "id": "xFuExNWd8FLA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_shape = classi._get_testing_shape_feature(testing_img_set, testing_lab_set, alig_im_test, alig_lan_test, diff_path_test)"
      ],
      "metadata": {
        "id": "5eBQ4v2x67VI"
      },
      "id": "5eBQ4v2x67VI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape = classi._get_training_shape_feature()"
      ],
      "metadata": {
        "id": "wqd8gT8Q69VU"
      },
      "id": "wqd8gT8Q69VU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "imgEnv-kernel",
      "language": "python",
      "name": "imgenv-kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}