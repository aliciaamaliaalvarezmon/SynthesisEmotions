{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filters\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np # NUMPY\n",
    "import pandas as pd # PANDAS\n",
    "import matplotlib.pyplot as plt # MATPLOTLIB\n",
    "import seaborn as sns # SEABORN\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2\n",
    "import os \n",
    "from skimage import filters\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks_of_folder(files_vector):\n",
    "    frames_landmarks_XYord = []\n",
    "    for i in np.sort(files_vector):\n",
    "        file_landmarks = get_landmarks_of_file(i)\n",
    "        frames_landmarks_XYord.append(file_landmarks)\n",
    "    frames_landmarks_XYord = np.asarray(frames_landmarks_XYord)\n",
    "    return frames_landmarks_XYord\n",
    "\n",
    "def get_landmarks_of_file(f):\n",
    "    data = pd.read_csv(f, header = None)\n",
    "    data.columns =['landmarks']\n",
    "    quantity_of_landmarks = data.size #68\n",
    "    data = data['landmarks'].str.split('   ', expand=True)\n",
    "    data.columns =['landmarks', 'landmarkX', 'landmarkY']\n",
    "    data =data.drop(columns=['landmarks'])\n",
    "    landmarkXY= pd.concat([data['landmarkX'], data['landmarkY'] ], ignore_index=True)\n",
    "    landmarkXY_array = landmarkXY.to_numpy(dtype=float)\n",
    "    return landmarkXY_array\n",
    "    \n",
    "\n",
    "def testimport():\n",
    "    print('estafuncando')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Cambiamos a [x1, y1, x2,y2]\n",
    "#Ahora lo hacemos en pares\n",
    "def intercalate_datavector(vector):    \n",
    "    x_coord = vector[0:int(len(vector)/2)]    \n",
    "    y_coord = vector[int(len(vector)/2):len(vector)]\n",
    "    output = []\n",
    "    for (a, b) in zip(x_coord, y_coord):\n",
    "        output.append(a)\n",
    "        output.append(b)     \n",
    "    return np.asarray(output)\n",
    "\n",
    "def intercalate_data_of_datavectors(vectors, pairs):\n",
    "    intercalated_data = []\n",
    "    for i in range(0, len(vectors)):\n",
    "        if (pairs == 0):\n",
    "            intercalated = intercalate_datavector(vectors.iloc[i])\n",
    "        else:\n",
    "            intercalated = intercalate_datavector_as_pairs(vectors.iloc[i])\n",
    "        intercalated_data.append(intercalated)\n",
    "    intercalated_data = np.asarray(intercalated_data)\n",
    "    return intercalated_data\n",
    "\n",
    "        \n",
    "def intercalate_data_of_vectors(vectors, pairs):\n",
    "    intercalated_data = []\n",
    "    for i in range(0, len(vectors)):\n",
    "        if (pairs == 0):\n",
    "            intercalated = intercalate_datavector(vectors[i])\n",
    "        else:\n",
    "            intercalated = intercalate_datavector_as_pairs(vectors[i])\n",
    "        intercalated_data.append(intercalated)\n",
    "    intercalated_data = np.asarray(intercalated_data)\n",
    "    return intercalated_data \n",
    "        \n",
    "def intercalate_datavector_as_pairs(vector):\n",
    "    x_coord = vector[0:int(len(vector)/2)]    \n",
    "    y_coord = vector[int(len(vector)/2):len(vector)]\n",
    "    output = []\n",
    "    for (a, b) in zip(x_coord, y_coord):\n",
    "        output.append(np.array([a,b]))            \n",
    "    return np.asarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def opa(a, b):# a, b matrices de puntos [[],[],[]] a and b need to have same dimension\n",
    "    aT = a.mean(0) # column mean [[1,2,3],[1,2,3]]<-(1,2,3)\n",
    "    bT = b.mean(0)\n",
    "    A = a - aT  #We rest aT and bT to center the matrix (translation)\n",
    "    B = b - bT\n",
    "    aS = np.sum(A * A)**.5 #to see the scale, we first calculate square of squared sum\n",
    "    bS = np.sum(B * B)**.5\n",
    "    A /= aS  #we standarize the scale.\n",
    "    B /= bS\n",
    "    U, _, V = np.linalg.svd(np.dot(B.T, A)) # we get the principal values of the mult for getting the rotation more close \n",
    "    aR = np.dot(U, V) #<-the rotation matrix is the multiplication of these 2\n",
    "    if np.linalg.det(aR) < 0: # we rotate to the positive side\n",
    "        V[1] *= -1\n",
    "        aR = np.dot(U, V)\n",
    "    aS = aS / bS  #to scale A most similar to B, we divide the scale by sSvale\n",
    "    aT-= (bT.dot(aR) * aS) #to get the translation in B coordinates, rest b mean rotated to A, and scaled to A \n",
    "    aD = (np.sum((A - B.dot(aR))**2) / len(a))**.5  # the square distance (error)of the A and B rotated after scale and translation\n",
    "    return aR, aS, aT, aD #return aROtation, aScale, aTranslation, Adifference\n",
    "        \n",
    "def gpa(v, n=-1): #v is vector of images, n indicates index to reflect\n",
    "    if n < 0: \n",
    "        p = avg(v)\n",
    "    else:\n",
    "        p = v[n]\n",
    "    l = len(v) #quantity of images\n",
    "    r, s, t, d = np.ndarray((4, l), object) #4 array of len l, each a matrix\n",
    "    for i in range(l):\n",
    "        r[i], s[i], t[i], d[i] = opa(p, v[i]) \n",
    "    return r, s, t, d#<-calculates the matrix and the error for all images, you use the one with the smallest one (can iterate this)\n",
    "\n",
    "def avg(v): #given a vector of images, calculates average shape of images \n",
    "    v_= np.copy(v)\n",
    "    l = len(v_) \n",
    "    R, S, T = [list(np.zeros(l)) for _ in range(3)]\n",
    "    for i, j in np.ndindex(l, l): #hace la daptacion de cada i para cada j\n",
    "        r, s, t, _ = opa(v_[i], v_[j]) \n",
    "        R[j] += np.arccos(min(1, max(-1, np.trace(r[:1])))) * np.sign(r[1][0]) \n",
    "        S[j] += s \n",
    "        T[j] += t #ajusta rot, s y translacion en todo j con todo i\n",
    "    for i in range(l):#hace el avg dado todo j\n",
    "        a = R[i] / l\n",
    "        r = [np.cos(a), -np.sin(a)], [np.sin(a), np.cos(a)]\n",
    "        v_[i] = v_[i].dot(r) * (S[i] / l) + (T[i] / l) \n",
    "    return v_.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_transform_image(reference_image, reference_landmarks, landmarks, images):\n",
    "    even = np.asarray(reference_landmarks[::2] )\n",
    "    odds = np.asarray( reference_landmarks[1::2] )\n",
    "    \n",
    "    \n",
    "    target_X_img = cv2.imread(reference_image,0)\n",
    "    lan =[]\n",
    "    for (a, b) in zip(even, odds):\n",
    "        lan.append(np.asarray([a,b]))  \n",
    "    \n",
    "    \n",
    "    even1 = np.asarray(landmarks[::2] )\n",
    "    odds1 = np.asarray( landmarks[1::2] )\n",
    "    lan1 =[]\n",
    "    for (a, b) in zip(even1, odds1):\n",
    "        lan1.append(np.asarray([a,b]))\n",
    "    \n",
    "    \n",
    "    X_pts = np.asarray(lan)\n",
    "    Y_pts = np.asarray(lan1)\n",
    "    \n",
    "\n",
    "    input_Y_img = cv2.imread(images)\n",
    "    \n",
    "    r,s,tr, d= opa(X_pts,Y_pts)\n",
    "    # Build and apply transform matrix...\n",
    "    # Note: for affine need 2x3 (a,b,c,d,e,f) form\n",
    "    R = np.eye(3)\n",
    "    R[0:2,0:2] = r\n",
    "    S = np.eye(3) * s\n",
    "    S[2,2] = 1\n",
    "    t = np.eye(3)\n",
    "    t[0:2,2] = tr\n",
    "    M = np.dot(np.dot(R,S),t.T).T\n",
    "    tr_Y_img = cv2.warpAffine(input_Y_img,M[0:2,:],(640,490))\n",
    "\n",
    "    \n",
    "    #transform deberia hacer el a[1] = a[1].dot(r) * s + t\n",
    "    Z_pts = Y_pts.dot(r) * s + tr\n",
    "    \n",
    "    return tr_Y_img, Z_pts \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def align_and_transform_datavector_of_images(reference_image, reference_landmarks, landmarks, images, pathi):        \n",
    "    transformed_landmarks_total = []        \n",
    "    for i in range(0, len(landmarks)):\n",
    "        print(images[i].split('/')[6])\n",
    "        tr_Y_img, Z_pts = align_and_transform_image(reference_image, reference_landmarks, landmarks.iloc[i], images[i])        \n",
    "        transformed_landmarks_total.append(Z_pts)\n",
    "        filename = 'warped_image_procustres_'+ str(images[i].split('/')[6]) \n",
    "        path = pathi\n",
    "        cv2.imwrite(os.path.join(path , filename), tr_Y_img)      \n",
    "    transformed_landmarks_total = np.asarray(transformed_landmarks_total)\n",
    "    return transformed_landmarks_total\n",
    "\n",
    "\n",
    "def align_and_transform_vector_of_images(reference_image, reference_landmarks, landmarks, images, pathi):        \n",
    "    transformed_landmarks_total = []        \n",
    "    for i in range(0, len(landmarks)):\n",
    "        tr_Y_img, Z_pts = align_and_transform_image(reference_image, reference_landmarks, landmarks[i], images[i])        \n",
    "        transformed_landmarks_total.append(Z_pts)\n",
    "        filename = 'warped_image_procustres_'+ str(images[i].split('/')[6]) \n",
    "        path = pathi\n",
    "        cv2.imwrite(os.path.join(path , filename), tr_Y_img)      \n",
    "    transformed_landmarks_total = np.asarray(transformed_landmarks_total)\n",
    "    return transformed_landmarks_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_frames_and_images(path,my_data):\n",
    "    first_images = []\n",
    "    first_frames = []\n",
    "    total = 0\n",
    "\n",
    "    current_neutral_index = 0\n",
    "    images_names = np.sort(os.listdir(path))\n",
    "    for i in range(0, len(images_names)):        \n",
    "        if int(images_names[i].split('_')[5].split('.')[0]) == 1:\n",
    "            first_frames.append(my_data.iloc[i])\n",
    "            first_images.append(path +'/'+ images_names[i])\n",
    "\n",
    "    first_frames = np.asarray( first_frames, dtype=object)\n",
    "    first_images = np.asarray( first_images, dtype=object)\n",
    "\n",
    "    return first_frames, first_images\n",
    "\n",
    "def get_first_frames(path,my_data):\n",
    "    return get_first_frames_and_images(path,my_data)[0]\n",
    "\n",
    "def get_first_images(path,my_data):\n",
    "    return get_first_frames_and_images(path,my_data)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_frames_and_images22(path):\n",
    "    first_images = []\n",
    "    first_frames = []\n",
    "    total = 0\n",
    "    \n",
    "    current_neutral_index = 0\n",
    "    images_names = np.sort(os.listdir(path))    \n",
    "    for i in range(0, len(images_names)):        \n",
    "        if int(images_names[i].split('_')[5].split('.')[0]) == 1:\n",
    "            first_frames.append(my_data.iloc[i])\n",
    "            first_images.append(path +'/'+ images_names[i])   \n",
    "\n",
    "    first_frames = np.asarray( first_frames, dtype=object)   \n",
    "    first_images = np.asarray( first_images, dtype=object) \n",
    "    \n",
    "    return first_frames, first_images\n",
    "\n",
    "def get_first_frames22(path):\n",
    "    return get_first_frames_and_images(path)[0]\n",
    "    \n",
    "def get_first_images22(path):\n",
    "    return get_first_frames_and_images(path)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this fuction changes order [x1..xn,y1..yn] to [x1, y1...xn, yn]\n",
    "def change_to_intercalate_order(frame_list):\n",
    "    intercalated_first_frames = []\n",
    "    for fframe in frame_list:\n",
    "        xaxis = fframe[0: int(len(frame_list[0])/2)]\n",
    "        yaxis = fframe[int(len(frame_list[0])/2):len(frame_list[0])]\n",
    "        lan =[]\n",
    "        for (a, b) in zip(xaxis, yaxis):\n",
    "            lan.append(np.asarray([a,b]))        \n",
    "        intercalated_first_frames.append( np.asarray(lan))\n",
    "    intercalated_first_frames = np.asarray(intercalated_first_frames)\n",
    "    return intercalated_first_frames\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "\n",
    "def feature_point_one(im):\n",
    "    point1 = im[18][0]\n",
    "    point2 = im[25][0]\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_two(im):\n",
    "    point1 = im[1][0]\n",
    "    point2 = im[15][0]\n",
    "    return np.abs(point2-point1)\n",
    "   \n",
    "def feature_point_three(im):\n",
    "    point1 = im[1][0]\n",
    "    point2 = im[14][0]\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_four(im):   \n",
    "    point1a = im[3][0]\n",
    "    point2a = im[13][0]        \n",
    "    point1b = im[4][0]\n",
    "    point2b = im[12][0]\n",
    "    point1 = (point1a + point1b)/2#mean\n",
    "    point2 = (point2a + point2b)/2\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_five(im):   \n",
    "    point1 = im[6][0]\n",
    "    point2 = im[10][0]    \n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_six(im):\n",
    "    point1 = im[31][0]\n",
    "    point2 = im[35][0]\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_seven(im):\n",
    "    point1 = im[61][0]\n",
    "    point2 = im[65][0]\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_eight(im):\n",
    "    point1a = im[37][0]\n",
    "    point2a = im[43][0]        \n",
    "    point1b = im[38][0]\n",
    "    point2b = im[44][0]\n",
    "    point1 = (point1a + point1b)/2#mean\n",
    "    point2 = (point2a + point2b)/2\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_nine(im):\n",
    "    point1 = im[42][0]\n",
    "    point2 = im[45][0]\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_ten(im):\n",
    "    point1 = im[39][0]\n",
    "    point2 = im[42][0]\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_eleven(im):\n",
    "    point1 = im[36][0]\n",
    "    point2 = im[39][0]\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_twelve(im):\n",
    "    point1 = im[27][1]\n",
    "    point2 = im[33][1]\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_thirteen(im):\n",
    "    point1 = im[33][1]\n",
    "    point2 = im[8][1]\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_fourteen(im):\n",
    "    point1a = im[38][1]\n",
    "    point2a = im[20][1]        \n",
    "    point1b = im[43][1]\n",
    "    point2b = im[23][1]\n",
    "    point1 = (point1a + point1b)/2#mean\n",
    "    point2 = (point2a + point2b)/2\n",
    "    return np.abs(point2-point1)\n",
    "\n",
    "def feature_point_fifteen(im):\n",
    "    point1 = im[37][1]\n",
    "    point2 = im[41][1]        \n",
    "    point3 = im[38][1]\n",
    "    point4 = im[40][1]     \n",
    "    point5 = im[43][1]\n",
    "    point6 = im[47][1]        \n",
    "    point7 = im[44][1]\n",
    "    point8 = im[46][1]\n",
    "    point_pair1 = np.abs(point2-point1)\n",
    "    point_pair2 = np.abs(point4-point3)\n",
    "    point_pair3 = np.abs(point6-point5)\n",
    "    point_pair4 = np.abs(point8-point7)\n",
    "    return np.mean(np.array([point_pair1, point_pair2, point_pair3, point_pair4]))\n",
    "    \n",
    "def feature_point_sixteen(im):\n",
    "    point1 = im[33][1]\n",
    "    point2 = im[56][1]\n",
    "    return  np.abs(point2-point1)\n",
    "\n",
    "def feature_point_seventeen(im):\n",
    "    point1 = im[67][1]\n",
    "    point2 = im[8][1]\n",
    "    return  np.abs(point2-point1)\n",
    "\n",
    "def feature_point_eighteen(im):\n",
    "    points = im[39:59]\n",
    "    hull = ConvexHull(points)   \n",
    "    return  hull.volume\n",
    "\n",
    "def feature_point_ninetotwentyone(im, imagen):\n",
    "        #Rectangulo actual en anterorior era siempre imagen 0. FIjarsefirst_frame[0][1][0]\n",
    "    left_cheek_x0 = int(im[1][0])#x de landmark 2\n",
    "    left_cheek_x1 = int(im[31][0])#\n",
    "    left_cheek_y0 = int(im[28][1])\n",
    "    left_cheek_y1 = int(im[2][1])\n",
    "\n",
    "    right_cheek_x0 = int(im[35][0])#x de landmark 2\n",
    "    right_cheek_x1 = int(im[15][0])#\n",
    "    right_cheek_y0 = int(im[28][1])\n",
    "    right_cheek_y1 = int(im[14][1])\n",
    "    \n",
    "    img = cv2.imread(imagen)\n",
    "    \n",
    "    imgall = img[left_cheek_y0: right_cheek_y1,left_cheek_x0:right_cheek_x1 ]\n",
    "    imgall =cv2.cvtColor(imgall, cv2.COLOR_BGR2GRAY)   \n",
    "    nineteen, twenty, twentyone = cheeks_function_with_nose(imgall, left_cheek_y0,left_cheek_x0)\n",
    "    return  nineteen, twenty, twentyone\n",
    "\n",
    "def feature_point_nineteen(im, imagen):\n",
    "    return feature_point_ninetotwentyone(im, imagen)[0]\n",
    "\n",
    "def feature_point_twenty(im, imagen):\n",
    "    return feature_point_ninetotwentyone(im, imagen)[1]\n",
    "\n",
    "def feature_point_twentyone(im, imagen):\n",
    "    return feature_point_ninetotwentyone(im, imagen)[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_points_options = {1 : feature_point_one,\n",
    "           2 : feature_point_two,\n",
    "           3 : feature_point_three,\n",
    "           4 : feature_point_four,\n",
    "           5 : feature_point_five,\n",
    "           6 : feature_point_six,\n",
    "           7 : feature_point_seven,\n",
    "           8 : feature_point_eight,\n",
    "           9 : feature_point_nine,\n",
    "           10 : feature_point_ten,\n",
    "           11 : feature_point_eleven,\n",
    "           12 : feature_point_twelve,\n",
    "           13 : feature_point_thirteen,\n",
    "           14 : feature_point_fourteen,\n",
    "           15 : feature_point_fifteen,\n",
    "           16 : feature_point_sixteen,\n",
    "           17 : feature_point_seventeen,\n",
    "           18 : feature_point_eighteen,\n",
    "           19 : feature_point_nineteen,\n",
    "           20 : feature_point_twenty,\n",
    "           21 : feature_point_twentyone,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def cheeks_function_with_nose(imgall, left_cheek_y0,left_cheek_x0):\n",
    "    #usamos sobel para sacar las derivadas\n",
    "    edgesx = cv2.Sobel(imgall, -1, dx=1, dy=0, ksize=1)\n",
    "    edgesy = cv2.Sobel(imgall, -1, dx=0, dy=1, ksize=1)\n",
    "#   edgesy = cv2.Sobel(imgleft, -1, dx=0, dy=1, ksize=1)    \n",
    "    output = np.zeros((len(edgesx),len(edgesx[0])))\n",
    "    \n",
    "    for i in range(0, len(edgesx)):\n",
    "        for j in range(0, len(edgesx[0])):\n",
    "            output[i][j] = math.sqrt(pow(edgesx[i][j],2) + pow(edgesy[i][j],2)+ pow(-1,2)) #raiz de suma de cuadrados y -1 al cuadradp\n",
    "\n",
    "    angle = np.zeros((len(edgesx),len(edgesx[0]),3))\n",
    "    \n",
    "    for i in range(0, len(edgesx)):\n",
    "        for j in range(0, len(edgesx[0])):\n",
    "            pos =np.array([edgesx[i][j], edgesy[i][j], -1]/output[i][j])\n",
    "            angle[i][j] = pos\n",
    "            \n",
    "    beta = np.array([0,0,1])    \n",
    "\n",
    "    #dados los angles, sacamos los coefficientes\n",
    "        \n",
    "    coff = np.zeros((len(edgesx),len(edgesx[0])))\n",
    "    for i in range(0, len(edgesx)):\n",
    "        for j in range(0, len(edgesx[0])):\n",
    "            coff[i][j] = 1- np.dot(angle[i][j],beta)\n",
    "\n",
    "    \n",
    "    coff = coff.flatten()\n",
    "    coff_sum = coff.sum() #lo usamos para center of mass\n",
    "    coff_average = coff.mean()#es el 19\n",
    "    \n",
    "    partial_sumx = []    \n",
    "    for i in range(0, len(edgesx)):#losy0debencambiarse arriba\n",
    "        partial_sumx.append(coff[i]*(left_cheek_y0+i))\n",
    "    \n",
    "    partial_sumy = []\n",
    "    for i in range(0, len(edgesx[0])):#losy0debencambiarse arriba\n",
    "        partial_sumy.append(coff[len(edgesx)+i]*(left_cheek_x0+i))  \n",
    "    \n",
    "\n",
    "  \n",
    "    center_of_massx = np.asarray(partial_sumx).sum()/coff_sum\n",
    "    center_of_massy = np.asarray(partial_sumy).sum()/coff_sum\n",
    "\n",
    "      \n",
    "    return coff_average, center_of_massx, center_of_massy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_point(im, number, imagen):\n",
    "    if number == 19 or number == 20 or number == 21 :\n",
    "        res = feature_points_options[number](im, imagen)\n",
    "    else:\n",
    "        res = feature_points_options[number](im)\n",
    "    return res\n",
    "\n",
    "def list_get_feature_number(lista, number, imagenlist):\n",
    "    res = []\n",
    "    for i in range(0, len(lista)):\n",
    "        fea_p = get_feature_point(lista[i], number, imagenlist[i])\n",
    "        res.append(fea_p)\n",
    "    return np.asarray(res,dtype= object)\n",
    "    \n",
    "def list_get_feature_point(lista, number, imagenlist):\n",
    "    res = list_get_feature_number(lista, number, imagenlist)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_the_structure_points_and_stack(first_frame, first_images):\n",
    "  first_feature_point = list_get_feature_point(first_frame, 1, first_images)\n",
    "  second_feature_point = list_get_feature_point(first_frame, 2, first_images)\n",
    "  third_feature_point = list_get_feature_point(first_frame, 3, first_images)\n",
    "  fourth_feature_point = list_get_feature_point(first_frame, 4, first_images)\n",
    "  five_feature_point = list_get_feature_point(first_frame, 5, first_images)\n",
    "  six_feature_point = list_get_feature_point(first_frame, 6, first_images)\n",
    "  seven_feature_point = list_get_feature_point(first_frame, 7, first_images)\n",
    "  eight_feature_point = list_get_feature_point(first_frame, 8, first_images)\n",
    "  nine_feature_point = list_get_feature_point(first_frame, 9, first_images)\n",
    "  ten_feature_point = list_get_feature_point(first_frame, 10, first_images)\n",
    "  eleven_feature_point = list_get_feature_point(first_frame, 11, first_images)\n",
    "  twelve_feature_point = list_get_feature_point(first_frame, 12, first_images)\n",
    "  thirteen_feature_point = list_get_feature_point(first_frame, 13, first_images)\n",
    "  fourteen_feature_point = list_get_feature_point(first_frame, 14, first_images)\n",
    "  fifteen_feature_point = list_get_feature_point(first_frame, 15, first_images)\n",
    "  sixteen_feature_point = list_get_feature_point(first_frame, 16, first_images)\n",
    "  seventeen_feature_point = list_get_feature_point(first_frame, 17, first_images)\n",
    "  eigtheen_feature_point = list_get_feature_point(first_frame, 18, first_images)\n",
    "  nineteen_feature = list_get_feature_point(first_frame, 19, first_images)\n",
    "  twenty_feature = list_get_feature_point(first_frame, 20, first_images)\n",
    "  twentyone_feature = list_get_feature_point(first_frame, 21, first_images)\n",
    "\n",
    "  structure_vector = first_feature_point\n",
    "  structure_vector = np.vstack((structure_vector, second_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, third_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, fourth_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, five_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, six_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, seven_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, eight_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, nine_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, ten_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, eleven_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, twelve_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, thirteen_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, fourteen_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, fifteen_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, sixteen_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, seventeen_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, eigtheen_feature_point))\n",
    "  structure_vector = np.vstack((structure_vector, nineteen_feature))\n",
    "  structure_vector = np.vstack((structure_vector, twenty_feature))\n",
    "  structure_vector = np.vstack((structure_vector, twentyone_feature))\n",
    "\n",
    "  return structure_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "def ComputeImageGradient(image, way):\n",
    "    if way == 0:\n",
    "        GradX = cv2.Sobel(image, ddepth=-1, dx=1, dy=0, borderType=cv2.BORDER_REFLECT)\n",
    "        GradY = cv2.Sobel(image, ddepth=-1, dx=0, dy=1, borderType=cv2.BORDER_REFLECT)\n",
    "        GradX[-1,:] = 0\n",
    "        GradY[:, -1] = 0 # Neumann boundary Condition\n",
    "    elif way == 1:\n",
    "        GradX, GradY = np.gradient(image)      \n",
    "        GradX[-1,:] = 0\n",
    "        GradY[:, -1] = 0 # Neumann boundary Condition     \n",
    "    elif way == 2: \n",
    "        Kx = -1*np.array([[-1,0,1]])\n",
    "        GradX = ndimage.convolve(img, Kx)\n",
    "        Ky = -1*np.array([[-1],[0],[1]])\n",
    "        GradY = ndimage.convolve(img, Ky)\n",
    "        GradX[-1,:] = 0\n",
    "        GradY[:, -1] = 0 # Neumann boundary Condition  \n",
    "    else:\n",
    "        print('not legal way')\n",
    "        GradY = np.zeros(image.shape)\n",
    "        GradX = np.zeros(image.shape)\n",
    "    return GradX, GradY\n",
    "\n",
    "\n",
    "#ComputeImageGradient(img,  0)\n",
    "#gradx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "def ProximalF_Star(img_p1,img_p2):\n",
    "    inp1 = img_p1.flatten()    \n",
    "    inp2 = img_p2.flatten()# (313600,1)\n",
    "    p_total =  np.dstack((inp1, inp2)) # queda (1,313600,2)\n",
    "    p_total = p_total[0]\n",
    "\n",
    "    partial = np.zeros(len(p_total))\n",
    "    \n",
    "    partial = np.asarray(list(map(LA.norm, p_total))) # cada el de p_total la norma 2\n",
    "    partial = np.asarray(list(map(lambda e: max(1.0,e), partial)))\n",
    "    \n",
    "    partial = partial.reshape(img_p1.shape)\n",
    "    \n",
    "    output_img_p1 = img_p1 / partial\n",
    "    output_img_p2 = img_p1 / partial \n",
    "\n",
    "\n",
    "    return output_img_p1, output_img_p2\n",
    "\n",
    "\n",
    "def ProximalG(img_u, img_g, landa, tau):\n",
    "    resta = img_u - img_g    \n",
    "    output_image = np.zeros(img_u.shape)\n",
    "    \n",
    "    maskH = resta > landa*tau\n",
    "    maskL = resta < -1*(landa*tau)\n",
    "    maskB = np.abs(resta) <= landa*tau \n",
    "\n",
    "    lander = np.zeros(img_u.shape)\n",
    "    lander.fill(landa*tau)\n",
    "    \n",
    "    landerA = lander *maskH\n",
    "    imageA = img_u * maskH\n",
    "    landerB = lander * maskL\n",
    "    imageB = img_u * maskL    \n",
    "   \n",
    "    \n",
    "    imageA = imageA - landerA\n",
    "    imageB = imageB + landerB\n",
    "    imageC = img_g * maskB\n",
    "    \n",
    "    output_image  = imageA +imageB + imageC    \n",
    "\n",
    "    return output_image\n",
    "\n",
    "def ProximalGD(img_u, img_g, landa, tau, resta):\n",
    "    #resta = img_u - img_g    \n",
    "    output_image = np.zeros(img_u.shape)\n",
    "    for i in range(0, len(img_u)):\n",
    "        for j in range(0, len(img_u[i])):            \n",
    "            if resta[i][j] > landa*tau :\n",
    "                output_image[i][j] = img_u[i][j] - (landa*tau)\n",
    "            elif resta[i][j] < -1*(landa*tau):\n",
    "                output_image[i][j] = img_u[i][j] + (landa*tau)\n",
    "            else: # si el modulo de la resta es menor a landa*tau\n",
    "                output_image[i][j] = img_g[i][j]\n",
    "    return output_image\n",
    "\n",
    "def ProximalGL(img_u, img_g, landa, tau):\n",
    "    resta = img_u - img_g    \n",
    "    output_image = np.zeros(img_u.shape)\n",
    "    for i in range(0, len(img_u)):\n",
    "        for j in range(0, len(img_u[i])):            \n",
    "            if resta[i][j] > landa*tau :\n",
    "                output_image[i][j] = img_u[i][j] - (landa*tau)\n",
    "            elif resta[i][j] < -1*(landa*tau):\n",
    "                output_image[i][j] = img_u[i][j] + (landa*tau)\n",
    "            else: # si el modulo de la resta es menor a landa*tau\n",
    "                output_image[i][j] = img_g[i][j]\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeDivergenceL(img_p1, img_p2, way=1, sp=[1,1]):\n",
    "    if way == 0 :\n",
    "        p1_dx =  cv2.Sobel(img_p1, ddepth=-1, dx=1, dy=0, borderType=cv2.BORDER_REFLECT)\n",
    "        p2_dy =  cv2.Sobel(img_p2, ddepth=-1, dx=0, dy=1, borderType=cv2.BORDER_REFLECT)\n",
    "        output_image = p1_dx + p2_dy\n",
    "    elif way == 1 :\n",
    "        width, height =img_p1.shape\n",
    "        inp1 = img_p1.flatten()\n",
    "        inp2 = img_p2.flatten()#no se si f o c\n",
    "        vector_field = np.array([inp1, inp2])\n",
    "        num_dims = 2        \n",
    "        output_image = np.ufunc.reduce(np.add, [np.gradient(vector_field[i], sp[i], axis =0) for i in range(num_dims)]) \n",
    "        output_image = output_image.reshape(width,height)        \n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chambolle_TV_L1(input_image, nb_iter, tau, sigma, landa, tetha, way):\n",
    "    # inicializacion\n",
    "    width, height  = input_image.shape\n",
    "    size_image = width * height\n",
    "    \n",
    "    u = input_image    \n",
    "    u_old = np.zeros(input_image.shape)    \n",
    "    p1 =  np.zeros(input_image.shape)# p = (p1, p2) o (px, py)\n",
    "    p2= np.zeros(input_image.shape)\n",
    "    img1 = np.zeros(input_image.shape)\n",
    "    img2 = np.zeros(input_image.shape)\n",
    "    GradX = np.zeros(input_image.shape)\n",
    "    GradY = np.zeros(input_image.shape)\n",
    "    \n",
    "    E = float('inf')\n",
    "    \n",
    "        \n",
    "    for it in range (1, nb_iter):\n",
    "        u_old = u\n",
    "        GradX, GradY = ComputeImageGradient(u, way) \n",
    "        im1 = p1+ sigma*GradX\n",
    "        im2 = p2 + sigma*GradY\n",
    "        p1, p2 = ProximalF_Star(im1,im2)        \n",
    "        div = ComputeDivergenceL(p1, p2)        \n",
    "        image1 = u + tau*div\n",
    "        u = ProximalG(image1, input_image, landa, tau)\n",
    "        u = u + tetha* (u - u_old)\n",
    "        \n",
    "        if( it % 10 == 0):\n",
    "            E_old = E\n",
    "            E = 0\n",
    "            GradX, GradY = ComputeImageGradient(u, way)\n",
    "            E += np.sum(landa * abs(u-input_image) + np.sqrt(GradX**2 + GradY**2))\n",
    "            E = E/size_image\n",
    "            if(abs(E-E_old) < 0.001): break\n",
    "    \n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartoonTexture_grey(input_image, landa, nb_iter_max=1000, tau=0.35, sigma=0.35, tetha =1.0, way =3):\n",
    "    \n",
    "    \n",
    "    cartoon = np.zeros(input_image.shape)    \n",
    "    cartoon = Chambolle_TV_L1(input_image, nb_iter_max, tau, sigma, landa, tetha, way)\n",
    "    \n",
    "    GradX = np.zeros(input_image.shape) \n",
    "    GradY = np.zeros(input_image.shape) \n",
    "    \n",
    "    BV_norm = 0.0\n",
    "    \n",
    "    GradX, GradY = ComputeImageGradient(cartoon, way)\n",
    "    \n",
    "    BV_norm += np.sum((GradX **2 + GradY **2)**(1/2))     \n",
    "    \n",
    "    \n",
    "    vLim = 20.0\n",
    "    \n",
    "    texture = np.zeros(input_image.shape)\n",
    "    \n",
    "    texture = input_image - cartoon\n",
    "    \n",
    "    texture  = (texture + vLim )*255.0 /(2.0* vLim)\n",
    "    maskL = texture  >= 0.0 # para que quede 0 cuando es menor a 0\n",
    "    texture = texture * maskL\n",
    "    texture[texture >255.0] =255.0 \n",
    "      \n",
    "    return cartoon, texture\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
